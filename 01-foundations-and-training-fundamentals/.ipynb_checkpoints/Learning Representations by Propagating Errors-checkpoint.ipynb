{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153d6875-dc17-455d-9cb3-82bf8e8faaf8",
   "metadata": {},
   "source": [
    "### Learning Representations by Propagating Errors (Backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f3e814-8c74-477f-89e2-7774ca68a09a",
   "metadata": {},
   "source": [
    "# Convincing myself of the delta rule for gradient descent\n",
    "$\\delta_{p}w_{ji} = \\eta(t_{pj}-o_{pj})i_{pi}$\n",
    "\n",
    "Where p is the pattern or the combination of input x's and output y,\n",
    "\n",
    "$\\eta$ is the learning rate\n",
    "\n",
    "$t_{pj}$ is the target of the j'th output for pattern p\n",
    "\n",
    "$o_{pj}$ is our prediction of the j'th output for pattern p\n",
    "\n",
    "$i_{pi}$ is the value of the ith element in the input for pattern p\n",
    "\n",
    "Imagine we're trying to learn AND\n",
    "\n",
    "|x1 | x2 | y |\n",
    "|-|-|-|\n",
    "|0|0|0|\n",
    "|0|1|0|\n",
    "|1|1|1|\n",
    "\n",
    "This is linearly seperable, so the network should be able to learn it. Let's initialize a simple network with one node and work through it by hand. It's a bit tedious to do this in latex / charts so I'll just put this image here.\n",
    "\n",
    "The delta rule does work and makes sense to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1eb5e827-7675-4a70-a7dc-c07b67b4c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try to write some code from scratch that allows us to construct these types of networks.\n",
    "# Note: this code isn't very good or safe, it's just to understand how it works / learns\n",
    "import random\n",
    "\n",
    "# network = Network(fan_in, fan_out)\n",
    "# network.forward(x)\n",
    "# network.update(x, y, learning_rate)\n",
    "\n",
    "class Network:\n",
    "    # need to store fan_in x fan_out connections\n",
    "    connections = {}\n",
    "    fan_in = 0\n",
    "    fan_out = 0\n",
    "    \n",
    "    def __init__(self, fan_in, fan_out):\n",
    "        for x in range(fan_in):\n",
    "            for y in range(fan_out):\n",
    "                self.connections[(x,y)] = random.random()\n",
    "        self.fan_in = fan_in\n",
    "        self.fan_out = fan_out\n",
    "\n",
    "    def threshold(self, val):\n",
    "        if val <= 0.5:\n",
    "            return 0\n",
    "        return 1\n",
    "                \n",
    "    def forward(self, input):\n",
    "        out = [0 for i in range(self.fan_out)]\n",
    "        for y in range(self.fan_out):\n",
    "            for x in range(self.fan_in):\n",
    "                out[y] += input[x] * self.connections.get((x, y))\n",
    "        out = [self.threshold(val) for val in out]\n",
    "        return out\n",
    "\n",
    "    def update(self, input, target, learning_rate=0.01):\n",
    "        # this is where we apply the generalized delta rule for each connection\n",
    "        forward_pass_vals = self.forward(input)\n",
    "\n",
    "        for y in range(self.fan_out):\n",
    "            for x in range(self.fan_in):\n",
    "                difference = target[y] - forward_pass_vals[y]\n",
    "                delta = learning_rate * difference * input[x]\n",
    "                # print(difference)\n",
    "                self.connections[(x,y)] = self.connections.get((x,y)) + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9bb5a06f-fd3d-4cdf-a291-3259e802a6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 0. Got: 1.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "\n",
      "After Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Now let's test if we can learn AND\n",
    "network = Network(2, 1)\n",
    "# \"training data\"\n",
    "data = [([0,0],0), ([0,1],0), ([1,0],0), ([1,1],1)]\n",
    "\n",
    "# Before training\n",
    "print(\"Before Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n",
    "\n",
    "# now let's \"train\" 100 times\n",
    "for i in range(100):\n",
    "    for example in data:\n",
    "        network.update(example[0], [example[1]])\n",
    "\n",
    "# After training\n",
    "print(\"\\nAfter Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e14da6f9-1a6e-4744-8c90-fc5802a1ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 0], should be 1. Got: 0.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "\n",
      "After Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 0], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Now let's test if we can learn OR\n",
    "network = Network(2, 1)\n",
    "# \"training data\"\n",
    "data = [([0,0],0), ([0,1],1), ([1,0],1), ([1,1],1)]\n",
    "\n",
    "# Before training\n",
    "print(\"Before Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n",
    "\n",
    "# now let's \"train\" 100 times\n",
    "for i in range(100):\n",
    "    for example in data:\n",
    "        network.update(example[0], [example[1]])\n",
    "\n",
    "# After training\n",
    "print(\"\\nAfter Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99701e02-2985-4ed9-8388-681e8da919ba",
   "metadata": {},
   "source": [
    "Cool so we can learn AND and OR, but the paper says, and it makes sense that we cannot learn XOR as it is not linearly seperable. So let's see what happens if we try to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f55132e1-77ff-407c-abc9-bb40b65992d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 1. Got: 0.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 0], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1], should be 0. Got: 1.00 --> \u001b[91mWRONG\u001b[0m\n",
      "\n",
      "After Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 1. Got: 0.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 0], should be 1. Got: 0.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 1], should be 0. Got: 1.00 --> \u001b[91mWRONG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Now let's test if we can learn XOR\n",
    "network = Network(2, 1)\n",
    "data = [([0,0],0), ([0,1],1), ([1,0],1), ([1,1],0)]\n",
    "\n",
    "# Before training\n",
    "print(\"Before Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n",
    "\n",
    "for i in range(100):\n",
    "    for example in data:\n",
    "        network.update(example[0], [example[1]])\n",
    "# After training\n",
    "print(\"\\nAfter Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c17c4-951e-4029-8894-d7af1717dc77",
   "metadata": {},
   "source": [
    "No matter how many times I run it, it will not learn. In the paper it mentions that with a single extra bit the network is able to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c9ac76d6-56c1-4de0-a345-f84a9b5bd0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "Input [0, 0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1, 0], should be 1. Got: 0.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 0, 0], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1, 1], should be 0. Got: 1.00 --> \u001b[91mWRONG\u001b[0m\n",
      "\n",
      "After Training\n",
      "Input [0, 0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1, 0], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 0, 0], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1, 1], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Now let's test if we can learn XOR with an extra bit\n",
    "network = Network(3, 1)\n",
    "data = [([0,0,0],0), ([0,1,0],1), ([1,0,0],1), ([1,1,1],0)]\n",
    "\n",
    "# Before training\n",
    "print(\"Before Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n",
    "\n",
    "# now let's \"train\" 100 times\n",
    "for i in range(1000):\n",
    "    for example in data:\n",
    "        network.update(example[0], [example[1]])\n",
    "\n",
    "# After training\n",
    "print(\"\\nAfter Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8784095-306b-45e6-8967-38179f23b0c9",
   "metadata": {},
   "source": [
    "Interesting, as the paper confirms it now \"learns\" the pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd54c30-8075-4290-8556-eb5b90fe0952",
   "metadata": {},
   "source": [
    "## Now let's see the derivation that proves that this rule is a gradient descent on the error\n",
    "\n",
    "If we specify the error metric for a single example as\n",
    "\n",
    "$\\frac{1}{2}\\sum(t_{pj}-o_{pj})^2$\n",
    "\n",
    "Then we want to show that the derivative of this is proportional to to the change specified in the rule above. (with the correct sign as well)\n",
    "\n",
    "To do this we can just do a bit of calculus with the chain rule. To do this we will show that the -derivative of the error with respect to the weight i for unit j is equal to:\n",
    "\n",
    "$\\delta_{p}i_{pi}$\n",
    "\n",
    "Where the delta term is simply the target y for unit j - our predicted y hat for unit j\n",
    "\n",
    "If we, for simplicities sake just imagine there's only one output and one input i\n",
    "\n",
    "$E_{p} = \\frac{1}{2}(y - (w * i))^2$\n",
    "\n",
    "Then if we apply the chain rule we get the derivative as\n",
    "\n",
    "$-(y-(w*i)) * i$\n",
    "\n",
    "Which if we're trying to minimize, we multiply by -1, giving us the term from rule from above!\n",
    "\n",
    "$i(y-\\hat y)$\n",
    "\n",
    "Now if we take a \"step\" in that direction, as given by the learning rate then we will minimize the error.\n",
    "\n",
    "However we did simplify here and assume there's only one input i and one output j for the math.\n",
    "\n",
    "Now what we can do however is observe that because the error metric is the summation of the errors on all the examples, we know the calculus sum rule means you can just add the derivatives together. So sum up the derivatives for the error with respect to each of the weights for all patterns and that gives us our overall gradient for the weight across the entire set. The interesting thing is that the authors note that if you update the weights after each pattern it's not a true gradient descent on E. I believe however the modern way is to compute the gradients for the entire input batch and sum the gradients together meaning it is a true descent on E.\n",
    "\n",
    "With no hidden units, this is a convex function, so it will always find the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e220aaeb-9f83-4951-9dd3-107f6312ea7f",
   "metadata": {},
   "source": [
    "Now we get to the real meat and potatoes of this paper. Their contribution is to show two things when applying this gradient descent algorithm in networks with hidden units.\n",
    "1. There is an efficient way to compute the derivatives for weights / biases anywhere in the network.\n",
    "2. The problem of getting stuck in a local minima (and not getting to the global minimum) doesn't really matter.\n",
    "\n",
    "To begin with we consider layered feedforward networks (the normal type essentially).\n",
    "\n",
    "We know that networks with hidden layers without any non-linear activations collapse into one large linear network we add nonlinear activation functions. These non linear activation functions that we add is an increasing and differentiable function that we feed the net output of a unit to (net output just means we perform the normal input vector @ weights).\n",
    "\n",
    "So maybe something like tanh(layer - 1's output @ unit j's weights).\n",
    "\n",
    "Now we do the derivation. The change in any of the weights in any place in the network needs to be proportional to the -derivative of the error with respect to that weight.\n",
    "\n",
    "How do we derive the derivative of the error with respect to a given weight? Well we know that between the error and the weight we feed it to that \"net\" function, which is just one term in the summation of the multiplication of the previous layers i'th output with this units i'th weight. (ignoring the activation function for now).\n",
    "\n",
    "Again then we apply the chain rule here to multiply the derivative of the error with respect to the output of that net function with the derivative of the net function with respect to a given weight.\n",
    "\n",
    "To get the derivative of the net function with respect to a given weight we know that there's only one term in the summation that has that given weight as a term, and it's a simple multiply with the i'th output from the layer before / the i'th input. Therefore we know that the derivative of the net function with respect to a given weight is just the i'th output from the layer before / the i'th input.\n",
    "\n",
    "We know that the derivative of the error with respect to the net is equal to that $\\delta_{p}i_{pi}$ term. This is because of how we define the error function. The error function is just $\\frac{1}{2} \\sum (t_{pj}-o_{pj})^2$, but $o_{pj}$ is the output of the net function passed into our non-linearity. Therefore our goal is to figure out what that $\\delta$ term should be, as we already know, at every point in our network, what the intput / $o_{pj}$ term is.\n",
    "\n",
    "There is a simple recursive computation (backprop) which can be implemented to propogate the error signals backwards.\n",
    "\n",
    "This is the crux of the paper here:\n",
    "\n",
    "To compute $\\delta_{pj}$ (the negative partial derivative of the error with respect to the net output of unit j for pattern p) which we want because once we have that we can quite easily compute the derivative for any weight in unit j using this mega derivative for unit j, we:\n",
    "\n",
    "Apply the chain rule to write this partial derivative as the product of two factors. We say that this mega derivative for unit j is:\n",
    "\n",
    "(1) the -partial derivative of error with respect to the output from unit j multiplied by (2) the partial derivative of the output from unit j with respect to the net function passed into our non linearity (the matrix multiply previous layer @ j's weights passed into sigmoid).\n",
    "\n",
    "First let's calculate (1) There are two cases for a unit j in a network with hidden layers. They are either in the last layer (output unit) or they are not. If they are an output unit then we follow the same logic as we would as before with the delta rule. The first term here is then simply $-(y - \\hat y)$ * nonlinearity(net_pj). The derivative of the error with respect to the output from unit j. The second term is, as we mentioned above the partial derivative of the output from unit j with respect to the net function passed into our non linearity. Therefore our second term is the derivative of that non linearity with the output \n",
    "\n",
    "Therefore our mega derivative ($\\delta_{pj}$)for the j'th output unit is $(y - \\hat y) * f'(net_{pj})$. For an output unit our mega derivative is the amount of error multiplied by the output from j.\n",
    "\n",
    "**Unit j is not an output unit but a hidden layer unit**\n",
    "\n",
    "Now what happens if unit j is not an output unit? This means that unit j is a hidden unit. That means that the output of this unit will not directly appear in the formula for the error, but we still want to calculate the derivative of the error with respect to this unit (to get the mega gradient j). The output of this unit j will influence the error through all the units k in the next layer that j sends its outputs to. In a fully connected network the output from this unit j will be multiplied by the j'th weight in all units k in the next layer. Therefore we use the chain rule again. For simplicity for now let's assume that we just have a chain of layers with one unit each (so there's no summation or anything).\n",
    "\n",
    "Remember we are trying to find the mega derivative for unit j which is made of two terms\n",
    "\n",
    "$\\delta_j = -\\frac{\\partial \\text{error}}{\\partial \\text{output of unit } j \\text{ (non\\_linearity(input @ weights))}} \\cdot \\frac{\\partial \\text{output of unit } j \\text{ (non\\_linearity(input @ weights))}}{\\partial \\text{input @ weights}}\n",
    "$\n",
    "\n",
    "The mega derivative becomes the overall output of our unit multiplied by the sum of (each of the units in the next layer's mega derivative multiplied by that units weight for our unit). The good news is that we already calculated all the next layer's mega derivatives :)\n",
    "\n",
    "But why is it this? Well let's think about it for a second. My (from the perspective of a unit who is not at an output layer) value gets consumed and is one tiny part in all of those lucky output units who get to influence the error directly. Yet to know how much I'm affecting the error I need to know two things. For each of those output units I affect, how are they affecting the error? If they go up / down does the error go up / down? That concept is captured by their mega derivative. The next thing I need to know is for each of those output units I affect, how important am I in terms of their output? That concept is captured by their weightage of my output.\n",
    "\n",
    "Therefore for me to calculate my own mega derivative (which tells me how the error will change if I change my net input (input @ my weights)), I need to multiply those two concepts together and sum all those derivatives up, to see if I change, how much the error is going to change, multiplied by the derivative of my own non-linear activation function with respect to my net input.\n",
    "\n",
    "**Let's look at the actual derivation of this now.**\n",
    "\n",
    "So we need to find the mega derivative\n",
    "\n",
    "$\\delta_{pj} = -\\frac{\\partial E_p}{\\partial \\text{net}_{pj}}$\n",
    "\n",
    "Where the E is the error for pattern p and the net_pj is the single value resulting from input @ weights.\n",
    "\n",
    "So imagine that we only have one output unit. Then our mega derivative would be the derivative of the error with respect to the output of the unit we feed into multiplied by the derivative of the output of the unit we feed into with respect to our output. That's saying that the error depends on the next unit, and the next unit depends on us. Therefore we can state this as:\n",
    "\n",
    "$$\\frac{\\partial E_p}{\\partial o_{pj}} =  \\frac{\\partial E_p}{\\partial \\text{net}_{pk}} \\cdot \\frac{\\partial \\text{net}_{pk}}{\\partial o_{pj}} $$\n",
    "\n",
    "So now let's start replacing and substituting things based on what we know. We know that the second term (figuring out how much the next unit changes based on us) is their weight for us. This is because we influence that equation by being multiplied by that weight and being added in. Therefore if we increase by 1, they will increase by the weight. Good! The first term we already calculated, which is their mega derivative.\n",
    "\n",
    "Cool so now we have this thing that we just derived :) but remember that $\\delta_{pj}$ is that thing we just derived multiplied by \n",
    "\n",
    "$\\frac{\\partial o_{pj}}{\\partial \\text{net}_{pj}}$\n",
    "\n",
    "or the derivative of our own output with respect to our net (input @ output). This however is just non_linearity'(input @ output)\n",
    "\n",
    "So finnally we get the result. To compute the mega derivative of a hidden unit we follow the formula:\n",
    "\n",
    "$$\\delta_{pj} = \\left( \\sum_{k \\in L_{next}} \\delta_{pk} w_{kj} \\right) \\cdot f'_j(\\text{net}_{pj})$$\n",
    "\n",
    "That was very tricky but I feel like it makes sense to me now! :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d9373-43d4-4b28-88b4-97a2542fe346",
   "metadata": {},
   "source": [
    "## Now let's try to implement this from scratch!\n",
    "I'm not going to bother making it super extensible or efficient or anything I just want it to work. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "0e4350da-16bb-417b-8a50-ab24601caa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try to write some code from scratch that allows us to construct these types of networks.\n",
    "# Note: this code isn't very good or safe, it's just to understand how it works / learns and to test\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# network = Network([fan_in, hidden layer n dims..., fan_out])\n",
    "# network.forward(x)\n",
    "# network.update(x, y, learning_rate)\n",
    "\n",
    "class Unit:\n",
    "    def __init__(self, fan_in):\n",
    "        self.weights = np.random.rand(fan_in) - 0.5\n",
    "        self.bias = 0.01\n",
    "        self.inputs = []\n",
    "        self.output = 0\n",
    "        self.learning_rate = 0.25\n",
    "        self.delta = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.inputs = np.array(x)\n",
    "        output = 1 / (1 + np.exp(-(x @ self.weights + self.bias)))\n",
    "        self.output = output\n",
    "        return output\n",
    "\n",
    "    def update_self_last(self, y):\n",
    "        error_signals = []\n",
    "        delta = (y - self.output) * self.output * (1 - self.output)\n",
    "        for i in range(len(self.weights)):\n",
    "            weight_input = self.inputs[i]\n",
    "            update = self.learning_rate * delta * weight_input\n",
    "            error_signal = delta * self.weights[i]\n",
    "            error_signals.append(error_signal)\n",
    "            self.weights[i] += update\n",
    "\n",
    "        self.bias += self.learning_rate * delta\n",
    "        \n",
    "        return error_signals\n",
    "        \n",
    "    def update_self(self, error_signals, unit_number):\n",
    "        error_signals_out = []\n",
    "        error_signal_sum = 0\n",
    "\n",
    "        for i in range(len(error_signals)):\n",
    "            error_signal_sum += error_signals[i][unit_number]\n",
    "        delta = error_signal_sum * self.output * (1 - self.output)\n",
    "        for i in range(len(self.weights)):\n",
    "            weight_input = self.inputs[i]\n",
    "            update = self.learning_rate * delta * weight_input\n",
    "            error_signal = delta * self.weights[i]\n",
    "            error_signals_out.append(error_signal)\n",
    "            self.weights[i] += update\n",
    "\n",
    "        self.bias += self.learning_rate * delta\n",
    "\n",
    "        return error_signals_out\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, units, fan_in):\n",
    "        self.units = []\n",
    "        for i in range(units):\n",
    "            self.units.append(Unit(fan_in))\n",
    "    def forward(self, x):\n",
    "        activations = []\n",
    "        for unit in self.units:\n",
    "            activations.append(unit.forward(x))\n",
    "        return activations\n",
    "        \n",
    "    def update_last_layer(self, y):\n",
    "        error_signals = []\n",
    "        for i in range(len(self.units)):\n",
    "            error_signals.append(self.units[i].update_self_last(y[i]))\n",
    "        return error_signals\n",
    "        \n",
    "    def update_self(self, error_signals):\n",
    "        error_signals_out = []\n",
    "        for i in range(len(self.units)):\n",
    "            error_signals_out.append(self.units[i].update_self(error_signals, i))\n",
    "        return error_signals_out\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layer_specs, input_dimensions):\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_specs)):\n",
    "            if(i == 0):\n",
    "                self.layers.append(Layer(layer_specs[i], input_dimensions))\n",
    "            else:\n",
    "                self.layers.append(Layer(layer_specs[i], layer_specs[i-1]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i].forward(x)\n",
    "        return x\n",
    "        \n",
    "    def update(self, x, y):\n",
    "        self.forward(x)\n",
    "        error_signals = self.layers[-1].update_last_layer(y)        \n",
    "        for i in reversed(range(len(self.layers) - 1)):\n",
    "            error_signals = self.layers[i].update_self(error_signals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ed17fe77-528c-4265-ae5d-63b5dfc2e027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "Input: [0, 0], Expected: 0, Got: 0.50 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [0, 1], Expected: 1, Got: 0.50 --> \u001b[91mWRONG\u001b[0m\n",
      "Input: [1, 0], Expected: 1, Got: 0.50 --> \u001b[91mWRONG\u001b[0m\n",
      "Input: [1, 1], Expected: 0, Got: 0.50 --> \u001b[92mCORRECT\u001b[0m\n",
      "\n",
      "After Training\n",
      "Input: [0, 0], Expected: 0, Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [0, 1], Expected: 1, Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [1, 0], Expected: 1, Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [1, 1], Expected: 0, Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "network = Network(layer_specs=[4, 4, 1], input_dimensions=2)\n",
    "data = [\n",
    "    ([0, 0], 0),\n",
    "    ([0, 1], 1),\n",
    "    ([1, 0], 1),\n",
    "    ([1, 1], 0),\n",
    "]\n",
    "\n",
    "# Evaluate before training\n",
    "print(\"Before Training\")\n",
    "for x, y in data:\n",
    "    output = network.forward(x)[0]\n",
    "    correct = abs(output - y) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f\"Input: {x}, Expected: {y}, Got: {output:.2f} --> {status}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(80000):\n",
    "    for x, y in data:\n",
    "        network.update(x, [y])\n",
    "\n",
    "# Evaluate after training\n",
    "print(\"\\nAfter Training\")\n",
    "for x, y in data:\n",
    "    output = network.forward(x)[0]\n",
    "    correct = abs(output - y) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f\"Input: {x}, Expected: {y}, Got: {output:.2f} --> {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3cdb2dc1-a210-4dba-bd81-8ef368401ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing it on one of their pattern examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "481b224b-1f4d-47f9-ad98-5fab56aa15b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing it on mnist lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e9bfe-0001-4532-bbdf-28a10cc1a611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
