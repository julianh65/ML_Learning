{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153d6875-dc17-455d-9cb3-82bf8e8faaf8",
   "metadata": {},
   "source": [
    "### Learning Representations by Propagating Errors (Backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f3e814-8c74-477f-89e2-7774ca68a09a",
   "metadata": {},
   "source": [
    "# Convincing myself of the delta rule for gradient descent\n",
    "$\\delta_{p}w_{ji} = \\eta(t_{pj}-o_{pj})i_{pi}$\n",
    "\n",
    "Where p is the pattern or the combination of input x's and output y,\n",
    "\n",
    "$\\eta$ is the learning rate\n",
    "\n",
    "$t_{pj}$ is the target of the j'th output for pattern p\n",
    "\n",
    "$o_{pj}$ is our prediction of the j'th output for pattern p\n",
    "\n",
    "$i_{pi}$ is the value of the ith element in the input for pattern p\n",
    "\n",
    "Imagine we're trying to learn AND\n",
    "\n",
    "|x1 | x2 | y |\n",
    "|-|-|-|\n",
    "|0|0|0|\n",
    "|0|1|0|\n",
    "|1|1|1|\n",
    "\n",
    "This is linearly seperable, so the network should be able to learn it. Let's initialize a simple network with one node and work through it by hand. It's a bit tedious to do this in latex / charts so I'll just put this image here.\n",
    "\n",
    "The delta rule does work and makes sense to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1eb5e827-7675-4a70-a7dc-c07b67b4c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try to write some code from scratch that allows us to construct these types of networks.\n",
    "# Note: this code isn't very good or safe, it's just to understand how it works / learns\n",
    "import random\n",
    "\n",
    "# network = Network(fan_in, fan_out)\n",
    "# network.forward(x)\n",
    "# network.update(x, y, learning_rate)\n",
    "\n",
    "class Network:\n",
    "    # need to store fan_in x fan_out connections\n",
    "    connections = {}\n",
    "    fan_in = 0\n",
    "    fan_out = 0\n",
    "    \n",
    "    def __init__(self, fan_in, fan_out):\n",
    "        for x in range(fan_in):\n",
    "            for y in range(fan_out):\n",
    "                self.connections[(x,y)] = random.random()\n",
    "        self.fan_in = fan_in\n",
    "        self.fan_out = fan_out\n",
    "\n",
    "    def threshold(self, val):\n",
    "        if val <= 0.5:\n",
    "            return 0\n",
    "        return 1\n",
    "                \n",
    "    def forward(self, input):\n",
    "        out = [0 for i in range(self.fan_out)]\n",
    "        for y in range(self.fan_out):\n",
    "            for x in range(self.fan_in):\n",
    "                out[y] += input[x] * self.connections.get((x, y))\n",
    "        out = [self.threshold(val) for val in out]\n",
    "        return out\n",
    "\n",
    "    def update(self, input, target, learning_rate=0.01):\n",
    "        # this is where we apply the generalized delta rule for each connection\n",
    "        forward_pass_vals = self.forward(input)\n",
    "\n",
    "        for y in range(self.fan_out):\n",
    "            for x in range(self.fan_in):\n",
    "                difference = target[y] - forward_pass_vals[y]\n",
    "                delta = learning_rate * difference * input[x]\n",
    "                # print(difference)\n",
    "                self.connections[(x,y)] = self.connections.get((x,y)) + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9bb5a06f-fd3d-4cdf-a291-3259e802a6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 0. Got: 1.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "\n",
      "After Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Now let's test if we can learn AND\n",
    "network = Network(2, 1)\n",
    "# \"training data\"\n",
    "data = [([0,0],0), ([0,1],0), ([1,0],0), ([1,1],1)]\n",
    "\n",
    "# Before training\n",
    "print(\"Before Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n",
    "\n",
    "# now let's \"train\" 100 times\n",
    "for i in range(100):\n",
    "    for example in data:\n",
    "        network.update(example[0], [example[1]])\n",
    "\n",
    "# After training\n",
    "print(\"\\nAfter Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e14da6f9-1a6e-4744-8c90-fc5802a1ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 0], should be 1. Got: 0.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "\n",
      "After Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 0], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Now let's test if we can learn OR\n",
    "network = Network(2, 1)\n",
    "# \"training data\"\n",
    "data = [([0,0],0), ([0,1],1), ([1,0],1), ([1,1],1)]\n",
    "\n",
    "# Before training\n",
    "print(\"Before Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n",
    "\n",
    "# now let's \"train\" 100 times\n",
    "for i in range(100):\n",
    "    for example in data:\n",
    "        network.update(example[0], [example[1]])\n",
    "\n",
    "# After training\n",
    "print(\"\\nAfter Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99701e02-2985-4ed9-8388-681e8da919ba",
   "metadata": {},
   "source": [
    "Cool so we can learn AND and OR, but the paper says, and it makes sense that we cannot learn XOR as it is not linearly seperable. So let's see what happens if we try to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f55132e1-77ff-407c-abc9-bb40b65992d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 1. Got: 0.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 0], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1], should be 0. Got: 1.00 --> \u001b[91mWRONG\u001b[0m\n",
      "\n",
      "After Training\n",
      "Input [0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1], should be 1. Got: 0.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 0], should be 1. Got: 0.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 1], should be 0. Got: 1.00 --> \u001b[91mWRONG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Now let's test if we can learn XOR\n",
    "network = Network(2, 1)\n",
    "data = [([0,0],0), ([0,1],1), ([1,0],1), ([1,1],0)]\n",
    "\n",
    "# Before training\n",
    "print(\"Before Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n",
    "\n",
    "for i in range(100):\n",
    "    for example in data:\n",
    "        network.update(example[0], [example[1]])\n",
    "# After training\n",
    "print(\"\\nAfter Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c17c4-951e-4029-8894-d7af1717dc77",
   "metadata": {},
   "source": [
    "No matter how many times I run it, it will not learn. In the paper it mentions that with a single extra bit the network is able to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c9ac76d6-56c1-4de0-a345-f84a9b5bd0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "Input [0, 0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1, 0], should be 1. Got: 0.00 --> \u001b[91mWRONG\u001b[0m\n",
      "Input [1, 0, 0], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1, 1], should be 0. Got: 1.00 --> \u001b[91mWRONG\u001b[0m\n",
      "\n",
      "After Training\n",
      "Input [0, 0, 0], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [0, 1, 0], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 0, 0], should be 1. Got: 1.00 --> \u001b[92mCORRECT\u001b[0m\n",
      "Input [1, 1, 1], should be 0. Got: 0.00 --> \u001b[92mCORRECT\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Now let's test if we can learn XOR with an extra bit\n",
    "network = Network(3, 1)\n",
    "data = [([0,0,0],0), ([0,1,0],1), ([1,0,0],1), ([1,1,1],0)]\n",
    "\n",
    "# Before training\n",
    "print(\"Before Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n",
    "\n",
    "# now let's \"train\" 100 times\n",
    "for i in range(1000):\n",
    "    for example in data:\n",
    "        network.update(example[0], [example[1]])\n",
    "\n",
    "# After training\n",
    "print(\"\\nAfter Training\")\n",
    "for example in data:\n",
    "    output = network.forward(example[0])[0]\n",
    "    correct = abs(output - example[1]) < 0.5\n",
    "    status = f\"\\033[92mCORRECT\\033[0m\" if correct else f\"\\033[91mWRONG\\033[0m\"\n",
    "    print(f'Input {example[0]}, should be {example[1]}. Got: {output:.2f} --> {status}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8784095-306b-45e6-8967-38179f23b0c9",
   "metadata": {},
   "source": [
    "Interesting, as the paper confirms it now \"learns\" the pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd54c30-8075-4290-8556-eb5b90fe0952",
   "metadata": {},
   "source": [
    "## Now let's see the derivation that proves that this rule is a gradient descent on the error\n",
    "\n",
    "If we specify the error metric for a single example as\n",
    "\n",
    "$\\frac{1}{2}\\sum(t_{pj}-o_{pj})^2$\n",
    "\n",
    "Then we want to show that the derivative of this is proportional to to the change specified in the rule above.\n",
    "\n",
    "To do this we can just do a bit of calculus with the chain rule.\n",
    "\n",
    "If we, for simplicities sake just imagine there's only one output and one input i\n",
    "\n",
    "$E_{p} = \\frac{1}{2}\\sum(y - (w * i))^2$\n",
    "\n",
    "Then if we apply the chain rule we get the derivative as\n",
    "\n",
    "$-i(y-(w*i))$\n",
    "\n",
    "Which if we're trying to minimize, we multiply by -1, giving us the term from rule from above!\n",
    "\n",
    "$i(y-\\hat y)$\n",
    "\n",
    "Now if we take a \"step\" in that direction, as given by the learning rate then we will minimize the error.\n",
    "\n",
    "However we did simplify here and assume there's only one input i and one output j for the math.\n",
    "\n",
    "Now what we can do however is observe that because the error metric is the summation of the errors on all the examples, we know the calculus sum rule means you can just add the derivatives together. So sum up the derivatives for the error with respect to each of the weights for all patterns and that gives us our overall gradient for the weight across the entire set. The interesting thing is that the authors note that if you update the weights after each pattern it's not a true gradient descent on E. I believe however the modern way is to compute the gradients for the entire input batch and sum the gradients together meaning it is a true descent on E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684ed71-76fc-4c33-8c35-ec7dd8b97fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
