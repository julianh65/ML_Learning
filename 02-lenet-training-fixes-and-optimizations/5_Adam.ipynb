{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4809a8-ebb6-4602-8606-619ffeaec40b",
   "metadata": {},
   "source": [
    "Starting off by watching [this](https://www.youtube.com/watch?v=NE88eqLngkg) video before I read the paper:\n",
    "\n",
    "One thing that I found quite illustrative was his analogy of momentum as imaginging the weights w as some sort of particle moving through the loss space then we think of $\\rho$ the momentum coefficient as being the mass of the particle, and the gradient term being some sort of impulse on a particle with the v term being the current \"velocity\" of the particle.\n",
    "\n",
    "## First Pass of Paper\n",
    "\n",
    "Quickly a definition of SGD: Instead of using all training data we select one or a small batch of training data and get the gradients from that and then do updates. This is faster, and adds a little bit of noise to get out of local minima.\n",
    "\n",
    "I've experienced myself how it's a bit hard sometimes to pick a good learning rate. It can depend on so many things, network size, data, what activations functions we use, network architecture etc. I've kind of been trial and error-ing to get a good value. SGD also has one global rate. I haven't experienced it myself but sometimes loss landscapes can have long narrow valleys where it oscilates back and forth across the walls.\n",
    "\n",
    "Before this there are 2 ideas that help to combat this. The first was all the way back in the backprop paper from Rumelhart, which was to add the momentum, a moverage average of past gradients. The second is adaptive learning rates, each parameter gets its own learning rate.\n",
    "\n",
    "Adam combines these two ideas and adds one important correction to make it work properly.\n",
    "\n",
    "#### Adam Algorithm\n",
    "As inputs we have:\n",
    "\n",
    "$\\alpha$ the learning rate (recommended 0.001)\n",
    "\n",
    "$\\beta_{1} \\beta_{2}$ the exponential decay rates (recommended 0.9 and 0.999 respectively)\n",
    "\n",
    "$f(\\theta)$ the loss function\n",
    "\n",
    "$\\theta_{0}$ the weights and biases\n",
    "\n",
    "We then initialize a few vectors\n",
    "\n",
    "m_0 with the same shape as theta as a vector of 0s, this is the moment vector for momentum\n",
    "\n",
    "v_0  with the same shape as theta as a vector of 0s, this is for the adaptive learning rates\n",
    "\n",
    "t = 0 as the timestep counter.\n",
    "\n",
    "This is the algorithm:\n",
    "```python\n",
    "t += 1\n",
    "g_t = gradient of loss with respect to parameters\n",
    "m_t = beta_1 * m_t-1 + (1 - beta_1) * g_t\n",
    "# this allows us to update our weighted moving average (momentum) of the gradients with our fresh ones.\n",
    "# 0.9 * old gradients + 0.1 * fresh gradients\n",
    "v_t = beta_2 * v_t-1 + (1 - beta_2) * g_t^2\n",
    "# this updates our estimate of the uncentered variance of the gradients\n",
    "# similar structure to m_t update\n",
    "# we square them to keep track of the magnitude of recent gradients for each parameter -- squaring them makes it sensitive\n",
    "# to large values, later on when we divide by sqrt vt that means if we're getting a lot of large gradients we'll kind of\n",
    "# \"slow down\" and the learning rate will shrink\n",
    "m_hat_t = m_t / (1 - beta_1 ^ t)\n",
    "# this is a bias correction step. We initialized our momentum to 0 so it will, at the start, be more biased to 0\n",
    "# this helps to correct for that, when we have a small t (early in the training) then we'll be dividing by a small number\n",
    "# making it larger, as t gets larger and larger it will just be the term over 1\n",
    "v_hat_t = v_t / (1 - beta_2 ^ t)\n",
    "# same correction as above but for the uncentered variance term\n",
    "theta_t = theta_t-1 - alpha * m_hat_t / (sqrt(v_hat_t) + epsilon)\n",
    "# parameter update\n",
    "# we set the parameters to be the learning rate multiplied by our special term\n",
    "# our special term is the m_hat_t, (the momentum bias corrected gradients) divided by the square root of the\n",
    "# v_hat_t our corrected uncentered variance plus a little epsilon so we don't divide by 0 :)\n",
    "```\n",
    "Notes:\n",
    "\n",
    "So this is kind of a smart way to combine things and to remove the bias. It makes a lot of sense to me.\n",
    "\n",
    "We have the momentum that kind of smooths the movement and gives it more stable direction so we don't oscilate wildly if there's a weird gradient for the batch. We also have the second order term that sets our \"speed\" correctly. If the gradients we're getting are consistently small, then we'll kind of speed up to cover more ground over this more flat area. If the gradients wer're getting are consistently large, we're in some steepish area so we should slow down so we don't jump all over the place.\n",
    "\n",
    "##### Description / details of algorithm\n",
    "\n",
    "An important property is the choice of stepsize. Let's assume epsilon = 0 for simplicity. Aside from just the alpha the core insight is what they call the SNR or signal to noise ratio. which is that special term we multiply by.\n",
    "\n",
    "The signal is our estimation of the true direction of the gradient, the noise that we divide by is the magnitude of the gradient's fluctuation.\n",
    "\n",
    "One part that confused me that I just understood: I was looking a lot at the \"noise term\" or the uncentered variances in isolation. And it seemed strange to me that for this term that is supposed to capture the fluctuations that a bunch of steep gradients in the same direction is interpreted the same as a bunch of steep gradients in different sign directions. One seems to fluctuate while the other seems to just be a steep gradient. That is to say [-30, 30, -30, 30] is interpreted the same as [30,30,30,30] because of the squaring. But what I realized is that you need to also consider how that would affect the m_t term. For the two situations, if they all agree then the momentum will quickly build up and the magnitude will stabalize and we'll get a large update the size of alpha. But in the case of the fluctuating ones the moving average will be around 0 and the magnitude will still be large. Therefore we'll get a very small update because that ratio will be small.\n",
    "\n",
    "So that leads to the natural question of why does the v_t term matter so much. Like from the above scenario you may think that the momentum term on top is more important. That goes to my previous point.\n",
    "\n",
    "Imagine now a gentle gradient something like [0.2, 0.2, 0.2] then the momentums will agree in direction, and the v term will be very small, making it a bigger step\n",
    "\n",
    "If though we have a gradient that's something like [+5, -4, +5, -4] then we'll have a small ish directional concensus, but because the gradients are so large it's a bit of a red flag and it will take smaller steps then. The net signal is weak and the ground is steep.\n",
    "\n",
    "Now let's look at the upper bound analysis:\n",
    "\n",
    "There are two scenarios for upper bounds, the more common one is that the special term will roughly equal +/- 1, meaning that the alpha is roughly the upper bound of the size you take in the parameter space. \n",
    "\n",
    "Why is this?\n",
    "\n",
    "We have the ratio of E[g] / sqrt(E[g^2]). This is the expected value / average value of g over the square root of the spread of the values of g. From statistics we know the definition of variance is E[(g - mean)^2] If we expand out the square term and rearrange we get the variance = E[g^2] - mean^2. Again rearranging we get E[g^2] (the second raw moment = variance + mean squared.\n",
    "\n",
    "We know that the variance must be zero or positive, therefore we know that the second raw moment or E[g^2] >= E[g]^2. We replace the E[g^2] with v hat t our estimate of it, and with m hat t squared, our estimate of E[g]^2. Thus if we do a bit of algebra we do this:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "E[g^2] \\ge (E[g])^2 \\\\\n",
    "\\hat{v}_t \\ge (\\hat{m}_t)^2 \\\\\n",
    "\\sqrt{\\hat{v}_t} \\ge \\sqrt{(\\hat{m}_t)^2} \\\\\n",
    "\\sqrt{\\hat{v}_t} \\ge |\\hat{m}_t| \\\\\n",
    "\\frac{\\sqrt{\\hat{v}_t}}{\\sqrt{\\hat{v}_t}} \\ge \\frac{|\\hat{m}_t|}{\\sqrt{\\hat{v}_t}} \\\\\n",
    "1 \\ge \\frac{|\\hat{m}_t|}{\\sqrt{\\hat{v}_t}} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Therefore that special term is always less than or equal to 1 (if the assumption that m hat and v hat are perfect estimators.\n",
    "\n",
    "Now what happens in the case of \"most severe case of sparsity: when a gradient has been zero at all timestamps except at the current timestep\"?\n",
    "\n",
    "I'll spare the algebra, but they show that the step size would be roughly:\n",
    "$$ \\alpha * \\frac{1 - \\beta_{1}}{\\sqrt{1-\\beta_{2}}}$$\n",
    "\n",
    "Using the default values given in the paper that's = 0.00316 which doesn't seem that bad? With an alpha of 0.001 that's around 3 times larger than alpha. This is apparently good because that means the parameter that has not received any gradients so far (or very little) gets a larger boost. This allows it to \"catch up\" and learn from rare events.\n",
    "\n",
    "#### Initialization Bias Correction\n",
    "\n",
    "Where do those corrections come from? The dividing by 1 - beta_2 or 1 - beta_2 ^ t.\n",
    "\n",
    "Conceptually, when the first gradients arrive, the moving averages are skewed towards that initial zero.\n",
    "\n",
    "Side note: I wonder after enough time steps is this just wasting computation? Like once t = 100 or something why not just stop doing this. I suppose that\n",
    "\n",
    "So the correction kind of \"boosts\" the signal at earlier steps. As the beta term < 1 gets raised to a higher and higher power it will get smaller and smaller, making us divide more by 1. They give a proof of this in the paper.\n",
    "\n",
    "#### Convergence Analysis\n",
    "\n",
    "They analyze its convergence using an online learning framework.\n",
    "\n",
    "Side reading on this:\n",
    "\n",
    "The online convex optimization framework is a method for analyzing stochastic optimizers. At each timestep t from 1 to T the algorithm will proposet a set of parameters. A convex cost function, the loss on a new batch is revealed. The algorithm incurs a cost on that. The goal of the algorithm is to minimize the total accumulated cost over all steps.\n",
    "\n",
    "Regret then, R(T), measures how much worse the algorithm performed compared to the best single set of paramters chosen in hindsight. It sums the differences between the per step cost and optimal fixed-point cost. However in the real world they can't calculate the regret, how would they know what the best parameters are when the search space is so large?\n",
    "\n",
    "A good algorithm will have a regret that grows slower than T. Then it will converge R(T) / T at T -> infinity to 0 regret.\n",
    "\n",
    "Great!\n",
    "\n",
    "The authors show their theoretical result of an upper bound on the regret for Adam. They give a proof for this in the appendix. Seems a bit complicated to understand.\n",
    "\n",
    "#### Related Work / Other Optimizers\n",
    "\n",
    "AdaGrad -- pretty similar but Adam is better, the difference is that AdaGrad sums all past squared gradients.\n",
    "\n",
    "RMSProp -- very similar again, two differences: one is that there's a difference in how they apply momentum, rms prop first normalizes the current gradient and then applies the momentum to the rescaled gradient. Adam also has the bias correction.\n",
    "\n",
    "##### Experiments\n",
    "\n",
    "They show how much better it is compared to other algorithms. Side note though SGD with Nesterov momentum looks pretty decent.\n",
    "\n",
    "It beats out other algorithms in:\n",
    "1. Logistic Regression\n",
    "2. MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b168a71-d93a-4639-ab7a-1000fcf4a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "### Try to quickly implement Adam into my training loop\n",
    "\n",
    "# compare tanh, relu, sigmoid and softplus on same training I had from lenet with MLP\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import torch\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "x_test = torch.from_numpy(x_test)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "print(len(x_train), len(x_test))\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "class Fully_Connected_Layer():\n",
    "    def __init__(self, nin, nout, activation_function):\n",
    "        self.w = torch.randn(nin, nout)\n",
    "        self.w.mul_(math.sqrt(2.0 / nin)) \n",
    "        self.w.requires_grad_()                 \n",
    "        self.b = torch.randn(nout, requires_grad=True)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = (x @ self.w) + self.b\n",
    "        if(self.activation_function == \"relu\"):\n",
    "            return torch.relu(act)\n",
    "        if(self.activation_function == \"tanh\"):\n",
    "            return torch.tanh(act)\n",
    "        if(self.activation_function == \"sigmoid\"):\n",
    "            return torch.sigmoid(act)\n",
    "        if(self.activation_function == \"softplus\"):\n",
    "            return torch.nn.Softplus()(act)\n",
    "        return act\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.w, self.b]\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self, nin, nouts, activation_function):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Fully_Connected_Layer(sz[i], sz[i+1], activation_function) for i in range(len(nouts)- 1)]\n",
    "        self.layers.append(Fully_Connected_Layer(sz[-2], sz[-1], \"none\"))\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "def loss_individual(model_outputs, target_label_index):\n",
    "    max_logit = torch.max(model_outputs)\n",
    "    shifted_logits = model_outputs - max_logit\n",
    "    exp = torch.exp(shifted_logits)\n",
    "    sum_exp = torch.sum(exp)\n",
    "    target_exp = exp[target_label_index]\n",
    "    softmax_target = target_exp / sum_exp\n",
    "\n",
    "    # negative log liklihood\n",
    "    eps = 1e-12\n",
    "    loss = -torch.log(softmax_target + eps)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "406c360c-8bd6-4f66-b8dd-f006dd239940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_Optimizer():\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.parameters = parameters\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters:\n",
    "                p -= self.lr * p.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.zero_()\n",
    "\n",
    "class Adam_Optimizer():\n",
    "    def __init__(self, parameters, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.lr = learning_rate\n",
    "        self.b1 = beta1\n",
    "        self.b2 = beta2\n",
    "        self.eps = epsilon\n",
    "        self.t = 0\n",
    "        self.m = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.v = [torch.zeros_like(p) for p in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.parameters):\n",
    "                self.m[i] = self.b1 * self.m[i] + (1 - self.b1) * p.grad\n",
    "                self.v[i] = self.b2 * self.v[i] + (1 - self.b2) * (p.grad**2)\n",
    "                \n",
    "                m_hat = self.m[i] / (1 - self.b1**self.t)\n",
    "                v_hat = self.v[i] / (1 - self.b2**self.t)\n",
    "                \n",
    "                p -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01037c01-7c52-4f53-a42d-708ee8724efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with optimizer: SGD ---\n",
      "epoch done...\n",
      "Finished SGD in 43.42 seconds\n",
      "\n",
      "--- Training with optimizer: Adam ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "optimizers = [\"SGD\", \"Adam\"] \n",
    "losses_by_optimizer = {}\n",
    "\n",
    "for opt_name in optimizers:\n",
    "    print(f\"\\n--- Training with optimizer: {opt_name} ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    lossi = []\n",
    "    mnist_mlp = MLP(28*28, [256, 256, 10], \"relu\")\n",
    "    \n",
    "    if opt_name == \"SGD\":\n",
    "        optimizer = SGD_Optimizer(mnist_mlp.parameters(), learning_rate=0.001)\n",
    "    else:\n",
    "        optimizer = Adam_Optimizer(mnist_mlp.parameters(), learning_rate=0.001)\n",
    "\n",
    "    lambda_val = 0.0001\n",
    "    print_reset_error_interval = 1000\n",
    "    cumulative_error = 0\n",
    "    epochs = 1\n",
    "    examples_per_epoch = 20000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(examples_per_epoch):\n",
    "            flattened = torch.flatten(x_train[i])\n",
    "            out = mnist_mlp(flattened)\n",
    "            \n",
    "            # L2 Regularization\n",
    "            weights_sum = sum((p**2).sum() for p in mnist_mlp.parameters() if p.dim() > 1)\n",
    "            \n",
    "            loss = loss_individual(out, y_train[i].item()) + lambda_val * weights_sum\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                cumulative_error += loss_individual(out, y_train[i].item()).item()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if (i > 0 and i % print_reset_error_interval == 0):\n",
    "                lossi.append(cumulative_error / print_reset_error_interval)\n",
    "                cumulative_error = 0\n",
    "        print(\"epoch done...\")\n",
    "\n",
    "    losses_by_optimizer[opt_name] = lossi\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"Finished {opt_name} in {duration:.2f} seconds\")\n",
    "\n",
    "# Plot all on the same chart\n",
    "for opt_name, losses in losses_by_optimizer.items():\n",
    "    plt.plot(losses, label=opt_name)\n",
    "\n",
    "plt.xlabel(f'Measurement Step (every {print_reset_error_interval} examples)')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Adam vs. SGD Loss Curves')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33275b8b-51dd-45a7-a4c4-f2ecc2c386c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML Learning)",
   "language": "python",
   "name": "ml_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
