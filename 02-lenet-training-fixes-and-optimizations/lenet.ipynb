{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c112e4-4f90-494a-9e1f-50545259c3e9",
   "metadata": {},
   "source": [
    "### Introduction Notes\n",
    "We are interested in minimizing the error of our function over the test set. The gap between the test set and train set is given approximately as:\n",
    "\n",
    "$E_{\\text{test}} - E_{\\text{train}} = k(h/P)^{\\alpha}$\n",
    "\n",
    "If possible we'd like this term to be lower.\n",
    "\n",
    "P is the number of training examples, so increasing P decreases this gap.\n",
    "h is a measure of the capacity, so as the capacity goes up this value goes up, which makes sense as it will likely have more capacity to overfit\n",
    "\n",
    "In practice this is minimized by minimizing $E_{\\text{train}} + \\beta H (W)$ which is just where we add regularization.\n",
    "\n",
    "---\n",
    "\n",
    "We minimize this by computing the gradients and then updating the weights based on these gradients.\n",
    "\n",
    "Popular procedure is SGD or the \"on-line update\" where we don't get the complete gradient of the training set, but we get the gradient on the basis of a single or small amount of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e4f41-b4bb-4a70-a490-71d19ffd5f06",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "One advantage of convolutional neural networks is that they are better at handling invariance with respect to translations or local distortions. This is because with a large MLP, it could learn the different features in different positions, but it would be very inefficient as it would need multiple little mini networks within the network to handle detecting the important patterns positioned at different locations.\n",
    "\n",
    "Another issue with fully connected MLPs is that there is kind of this loss of information. The variables can be fed in in any fixed order and the outcome won't change. The information going into an MLP is just a 1d vector of numbers. Yet obviously images have a strong and important 2D structure.\n",
    "\n",
    "Convolutional networks use \"local receptive fields\" or patches. They extract the elementary features from them such as edges, corners etc... These features are then combined by subsequent convolutional layers in order to detect higher-order features.\n",
    "\n",
    "Because feature detectors that are useful on one part of the image are likely to be useful for the entire image, we use the single patch that has one set of weights to go across the entire image.\n",
    "\n",
    "Units in a layer are organized into planes where all the units share the same set of weights. What this means essentially is that we can kind of think of it as sliding a window of the same weights across the input. This would be opposed to, maybe in the bottom right we have a set of weights that detects a corner here and another set of weights in the top left that detects this corner here.\n",
    "\n",
    "This does not mean that each individual feature map has the same set of weights across them. That would be quite duplicative for no reason.\n",
    "\n",
    "In LeNet-5 the first layer has 6 planes or feature maps. A unit in a feature map has 25 inputs connected to a 5x5 area in the input. A sequential implementation (scanning across) would slide this 5x5 set of weights across the input and multiply each by the weight and then add the bias and squash at the end.\n",
    "\n",
    "Once a feature is detected, the location becomes less important, what's important is the position relative to other features.\n",
    "\n",
    "To reduce the precision with which the positions are encoded there is sub-sampling of layers which they use averaging but you could also use max pooling or something. This helps to reduce sensitivity to shifts and distortions, also may help with overfitting? Not sure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f9f6d-1a33-46d8-b659-4f730037d28d",
   "metadata": {},
   "source": [
    "Before I go into implementing CNNs I want to rework my code from the previous backprop notebook.\n",
    "\n",
    "I'm going to, based on Karpathy's micrograd, quickly write up / copy over the code so that I can have an autograd engine that implements .backwards. From there I'll add code on top of this that implements the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa616d-40ad-4fce-8df3-4692248a49b4",
   "metadata": {},
   "source": [
    "Because I want to practice this, I used o3 to turn the implementation of Value into an assignment with methods that I should implement myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107be411-a995-4d9e-a73a-9a5c907749c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from typing import Iterable, Union\n",
    "import math\n",
    "Number = Union[int, float]\n",
    "\n",
    "\n",
    "class Value:\n",
    "    \"\"\"A node in a dynamically‑built computation graph.\"\"\"\n",
    "\n",
    "    # -------------------------------------------------------------- T1\n",
    "    def __init__(self, data: Number, _children: Iterable[\"Value\"] = (), _op: str = \"\"):\n",
    "        \"\"\"Create a **leaf** (when ``_children`` is empty) or an **operation\n",
    "        result** (when created inside an operator).\n",
    "\n",
    "        What this method must do\n",
    "        ------------------------\n",
    "        1. Store ``data`` – the wrapped scalar (float/int).\n",
    "        2. Create ``grad`` and set it to **0.0**.  We accumulate partial\n",
    "           derivatives here during back‑prop.\n",
    "        3. Track the immediate predecessors with ``self._prev = set(_children)``.\n",
    "           We use a **set** so look‑ups are O(1) and there are no duplicates.\n",
    "        4. Save ``_op`` – a short string describing *how* this node was\n",
    "           produced (\"+\", \"*\", \"ReLU\", etc.).  It is *only* for graphviz /\n",
    "           debugging convenience.\n",
    "        5. Initialise ``self._backward`` to a **do‑nothing** lambda.  Each\n",
    "           operator will later *replace* this with a closure that knows\n",
    "           how to distribute ``out.grad`` to its parents.\n",
    "\n",
    "        Why these pieces?\n",
    "        -----------------\n",
    "        * Automatic differentiation works by walking the graph from the\n",
    "          output back to the leaves, so every node needs a list of its\n",
    "          parents (``_prev``) **and** the rule to push its gradient back\n",
    "          (``_backward``).\n",
    "        * ``grad`` starts at 0 so multiple downstream paths can safely\n",
    "          *accumulate* into it.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self.prev = set(_children)\n",
    "        self._op = _op\n",
    "\n",
    "    def __add__(self, other: Union[\"Value\", Number]):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other: Union[\"Value\", Number]):\n",
    "        \"\"\"self * other.\"\"\"\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    def __pow__(self, other: Union[\"Value\", Number]):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data ** other.data, (self, other), '**')\n",
    "    \n",
    "        def _backward():\n",
    "            self.grad += other.data * (self.data ** (other.data - 1)) * out.grad\n",
    "            self.grad += 0 \n",
    "            if self.data > 0:\n",
    "                other.grad += math.log(self.data) * out.data * out.grad\n",
    "    \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def relu(self):\n",
    "        \"\"\"ReLU activation: max(0, x).\"\"\"\n",
    "        out = Value(max(self.data, 0), (self,), \"relu\")\n",
    "        def _backward():\n",
    "            self.grad += max(0, out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Compute ``d(output)/d(node)`` for *every* ``node`` that influences\n",
    "        this Value (call it *out*).\n",
    "\n",
    "        Behaviour overview\n",
    "        ------------------\n",
    "        *The chain rule tells us we must process nodes **in reverse\n",
    "        topological order** – children **before** parents – so that when\n",
    "        we reach a node, the gradients flowing into it from all of its\n",
    "        consumers have already been accumulated.*\n",
    "\n",
    "        Implementation recipe\n",
    "        ---------------------\n",
    "        1. **Topological sort**\n",
    "           Depth‑first search starting from ``self`` collects nodes in a\n",
    "           list ``topo`` such that parents appear **before** children.\n",
    "        2. **Seed the output**\n",
    "           A node’s gradient with respect to itself is 1, so set\n",
    "           ``self.grad = 1.0``.\n",
    "        3. **Reverse sweep**\n",
    "           Iterate ``for v in reversed(topo): v._backward()``.  Each\n",
    "           stored ``_backward`` closure adds its *local* contribution to\n",
    "           ``child.grad``.\n",
    "        \"\"\"\n",
    "        # so we start from my node and go backwards calling backwards on all nodes from\n",
    "        # root me, all the way to the leaves\n",
    "\n",
    "        self.grad = 1\n",
    "        topographical_ancestors = []\n",
    "        seen = set()\n",
    "        def recurse(node):\n",
    "            topographical_ancestors.append(node)\n",
    "            seen.add(node)\n",
    "            if(node.prev):\n",
    "                for node in node.prev:\n",
    "                    if(node not in seen):\n",
    "                        recurse(node)\n",
    "        recurse(self)\n",
    "        for node in topographical_ancestors:\n",
    "            node._backward()\n",
    "            \n",
    "    def __rpow__(self, base: Union[\"Value\", Number]):\n",
    "        base = base if isinstance(base, Value) else Value(base)\n",
    "        out = Value(base.data ** self.data, (base, self), '**')\n",
    "    \n",
    "        def _backward():\n",
    "            base.grad += self.data * (base.data ** (self.data - 1)) * out.grad\n",
    "            self.grad += math.log(base.data) * out.data * out.grad\n",
    "    \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def log(self):\n",
    "        out = Value(math.log(self.data), (self,), 'log')\n",
    "        def _backward():\n",
    "            self.grad += (1 / self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other: Union[\"Value\", Number]):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other: Union[\"Value\", Number]):\n",
    "        return other + (-self)\n",
    "\n",
    "    def __truediv__(self, other: Union[\"Value\", Number]):\n",
    "        return self * other ** -1\n",
    "\n",
    "    def __rtruediv__(self, other: Union[\"Value\", Number]):\n",
    "        return other * self ** -1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data)\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "    \n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "    \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc63ca96-74dc-42f7-848b-8400cb2ed662",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the neuron, layer and MLP\n",
    "import random\n",
    "class Module:\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "    def __init__(self, nin, nonlinearity):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        if(self.nonlinearity == \"relu\"):\n",
    "            return act.relu()\n",
    "        return act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    pass\n",
    "    \n",
    "class Fully_Connected_Layer(Module):\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts, nonlinearity):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Fully_Connected_Layer(sz[i], sz[i+1], nonlinearity=nonlinearity) for i in range(len(nouts)- 1)]\n",
    "        self.layers.append(Fully_Connected_Layer(sz[-2], sz[-1], nonlinearity=\"none\"))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0b59943-f7f8-451b-be47-250c3597a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "2.2032093392464085\n",
      "0.21252774821545908\n",
      "0.15229204764947635\n",
      "0.12024741910287676\n",
      "\n",
      "--- Evaluation on Adder Problem ---\n",
      "Input: [0, 0, 0, 0], Expected: [0, 0, 0], Got_Bits: [0, 1, 0] --> \u001b[91mWRONG\u001b[0m\n",
      "Input: [0, 0, 0, 1], Expected: [0, 0, 1], Got_Bits: [0, 1, 1] --> \u001b[91mWRONG\u001b[0m\n",
      "Input: [0, 0, 1, 0], Expected: [0, 1, 0], Got_Bits: [0, 1, 0] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [0, 0, 1, 1], Expected: [0, 1, 1], Got_Bits: [0, 1, 1] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [0, 1, 0, 0], Expected: [0, 0, 1], Got_Bits: [0, 0, 1] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [0, 1, 0, 1], Expected: [0, 1, 0], Got_Bits: [1, 0, 0] --> \u001b[91mWRONG\u001b[0m\n",
      "Input: [0, 1, 1, 0], Expected: [0, 1, 1], Got_Bits: [0, 1, 1] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [0, 1, 1, 1], Expected: [1, 0, 0], Got_Bits: [1, 0, 0] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [1, 0, 0, 0], Expected: [0, 1, 0], Got_Bits: [0, 1, 0] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [1, 0, 0, 1], Expected: [0, 1, 1], Got_Bits: [0, 1, 1] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [1, 0, 1, 0], Expected: [1, 0, 0], Got_Bits: [1, 0, 0] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [1, 0, 1, 1], Expected: [1, 0, 1], Got_Bits: [1, 0, 1] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [1, 1, 0, 0], Expected: [0, 1, 1], Got_Bits: [0, 1, 1] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [1, 1, 0, 1], Expected: [1, 0, 0], Got_Bits: [1, 0, 0] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [1, 1, 1, 0], Expected: [1, 0, 1], Got_Bits: [1, 0, 1] --> \u001b[92mCORRECT\u001b[0m\n",
      "Input: [1, 1, 1, 1], Expected: [1, 1, 0], Got_Bits: [1, 1, 0] --> \u001b[92mCORRECT\u001b[0m\n",
      "\n",
      "Overall Accuracy: 81.25% (13/16)\n"
     ]
    }
   ],
   "source": [
    "## test micrograd on simple excercise from past paper like the adder problem\n",
    "data_adder = [\n",
    "    ([0,0,0,0], [0,0,0]), ([0,0,0,1], [0,0,1]), ([0,0,1,0], [0,1,0]), ([0,0,1,1], [0,1,1]),\n",
    "    ([0,1,0,0], [0,0,1]), ([0,1,0,1], [0,1,0]), ([0,1,1,0], [0,1,1]), ([0,1,1,1], [1,0,0]),\n",
    "    ([1,0,0,0], [0,1,0]), ([1,0,0,1], [0,1,1]), ([1,0,1,0], [1,0,0]), ([1,0,1,1], [1,0,1]),\n",
    "    ([1,1,0,0], [0,1,1]), ([1,1,0,1], [1,0,0]), ([1,1,1,0], [1,0,1]), ([1,1,1,1], [1,1,0])\n",
    "]\n",
    "\n",
    "adder_mlp = MLP(4, [16, 16, 3], \"relu\")\n",
    "learning_rate = 0.02\n",
    "epochs = 200\n",
    "print(\"running\")\n",
    "for epoch in range(epochs):\n",
    "    average_loss_cum = Value(0)\n",
    "    for x,y in data_adder:\n",
    "        adder_mlp.zero_grad()\n",
    "        out=adder_mlp(x)\n",
    "        loss = Value(0)\n",
    "        for i in range(len(y)):\n",
    "            loss += (1/2) * ((Value(y[i]) - out[i]) ** 2)\n",
    "        loss.backward()\n",
    "        average_loss_cum += loss\n",
    "        for param in adder_mlp.parameters():\n",
    "            param.data -= learning_rate * param.grad\n",
    "    if(epoch % 50 == 0):\n",
    "        print(average_loss_cum / len(data_adder))\n",
    "\n",
    "# evaluation from last time\n",
    "print(f\"\\n--- Evaluation on Adder Problem ---\")\n",
    "total_samples = len(data_adder)\n",
    "correct_predictions = 0\n",
    "\n",
    "for x_sample, y_true_list in data_adder:\n",
    "    out = adder_mlp(x_sample)\n",
    "    predicted_bits = [1 if o.data > 0.5 else 0 for o in out]\n",
    "    is_sample_correct = (predicted_bits == y_true_list)\n",
    "    \n",
    "    if is_sample_correct:\n",
    "        correct_predictions += 1\n",
    "        \n",
    "    status_color = \"\\033[92mCORRECT\\033[0m\" if is_sample_correct else \"\\033[91mWRONG\\033[0m\"\n",
    "    \n",
    "    print(f\"Input: {x_sample}, Expected: {y_true_list}, Got_Bits: {predicted_bits} --> {status_color}\")\n",
    "\n",
    "accuracy = (correct_predictions / total_samples) * 100\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.2f}% ({correct_predictions}/{total_samples})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08600657-7ed6-4c4e-9464-42bcac3420e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "## bring in MNIST and setup some convenience functions\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Optionally normalize (pixel values between 0 and 1)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert to plain lists (if you don't want NumPy arrays)\n",
    "x_train_list = x_train.tolist()\n",
    "y_train_list = y_train.tolist()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31e2352e-bcad-40cf-a6bf-322ea2976f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's work with a smaller set\n",
    "# 28 x 28 examples\n",
    "examples = 100\n",
    "x = x_train_list[:examples]\n",
    "y = y_train_list[:examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a79106-9a0c-4b9c-8226-d8a904c6550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to visualize an example\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_mnist_example(image, pred_label=None, true_label=None):\n",
    "    img = np.array(image)\n",
    "    # Set up label text\n",
    "    title = \"\"\n",
    "    if true_label is not None:\n",
    "        title += f\"True: {true_label}  \"\n",
    "    if pred_label is not None:\n",
    "        title += f\"Pred: {pred_label}\"\n",
    "\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title, fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2829b05a-5902-451f-a369-bdd14e60b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flat = []\n",
    "for i in x:\n",
    "    acc = []\n",
    "    for j in i:\n",
    "        acc += j\n",
    "    x_flat.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16dd4b26-8a98-45c1-a5b2-d6082ae45d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 2.71828\n",
    "\n",
    "def loss_individual(model_outputs, target_label_index):\n",
    "    max_output = max(model_outputs, key=lambda v: v.data if hasattr(v, 'data') else float(v))\n",
    "    shifted_outputs = [val - max_output for val in model_outputs]\n",
    "\n",
    "    exps = [Value(math.e) ** val for val in shifted_outputs]\n",
    "    sum_exps = Value(0)\n",
    "    for val in exps:\n",
    "        sum_exps += val\n",
    "\n",
    "    target_exp = exps[target_label_index]\n",
    "    normalized_target = target_exp / sum_exps\n",
    "\n",
    "    loss = -normalized_target.log()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa1cd7ac-021b-44c9-bca5-8d4bc3537362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "55050\n",
      "90.19601231180822\n",
      "57.80504646856195\n",
      "45.80947132662187\n",
      "65.92117384111525\n",
      "49.03913192907086\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "## Test running just a fully connected network on mnist with yann le cunns as well as cross\n",
    "## test micrograd on simple excercise from past paper like the adder problem\n",
    "lossi = []\n",
    "mnist_mlp = MLP(28*28, [64, 64, 10], \"relu\")\n",
    "learning_rate = 0.005 #3e-4 per karpathy's love lol\n",
    "epochs = 5\n",
    "print(\"running\")\n",
    "print(len(mnist_mlp.parameters()))\n",
    "for epoch in range(epochs):\n",
    "    average_loss_cum = Value(0)\n",
    "    for i in range(len(x_flat)):\n",
    "        mnist_mlp.zero_grad()\n",
    "        out=mnist_mlp(x_flat[i])\n",
    "        loss = loss_individual(out, y[i])\n",
    "        loss.backward()\n",
    "        average_loss_cum += loss\n",
    "        for param in mnist_mlp.parameters():\n",
    "            param.data -= learning_rate * param.grad\n",
    "    print(average_loss_cum / len(x_flat))\n",
    "    lossi.append(average_loss_cum / len(x_flat))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5492c34-1ee4-4416-b3aa-e13ac52e94d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test it on 10 examples\n",
    "test_number = 20\n",
    "# helper function to visualize an example\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def pick_label(out):\n",
    "    largest = 0\n",
    "    index = 0\n",
    "    for i in range(len(out)):\n",
    "        if(out[i].data > largest):\n",
    "            largest = out[i].data\n",
    "            index = i\n",
    "    return index\n",
    "        \n",
    "for i in range(test_number):\n",
    "    forward_label = mnist_mlp(x_flat[i])\n",
    "    show_mnist_example(x[i], pred_label=pick_label(forward_label), true_label=y[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "896e1b52-4924-48f8-9402-c82382dbc81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating...\n",
      "calculating...\n",
      "calculating...\n",
      "calculating...\n",
      "calculating...\n",
      "calculating...\n",
      "calculating...\n",
      "calculating...\n",
      "calculating...\n",
      "calculating...\n",
      "Accuracy: 180.00%\n"
     ]
    }
   ],
   "source": [
    "# get correct percentage on all examples\n",
    "correct = 0\n",
    "\n",
    "for i in range(len(x_flat)):\n",
    "    forward_label = mnist_mlp(x_flat[i])\n",
    "    pred = pick_label(forward_label)\n",
    "    true = y[i]\n",
    "\n",
    "    if(i % 10 == 0):\n",
    "        print(\"calculating...\")\n",
    "    \n",
    "    if pred == true:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = (correct / len(x_flat) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ee72d7e-14bf-40ca-862b-4b56bc4c01cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / len(x_flat)) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e2a7a-977d-4ea9-97c6-ade8f632b429",
   "metadata": {},
   "source": [
    "okay this takes years and isn't even very good lol, but I guess for just running a random MLP for 5 minutes, not tuning any hyper parameters, making it extremely small, it's not horrible. Show's the power of MLPs! 36% accuracy when a random guesser would be 10% right\n",
    "\n",
    "I guess we're simulating the tech from that era in terms of cpu and how slow everything is lmao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332d931-be91-408a-a52e-84159a12348f",
   "metadata": {},
   "source": [
    "So what does a single convolutional cell do? For simplicity I'm going to ignore stride and padding here for now.\n",
    "\n",
    "We have a filter, which in this case will be 2D. The filter will have a height and a width. It will also have a bias term.\n",
    "\n",
    "The filter is then slide across the input tensor. So it will slide over (input_width - filter_width) * (input_height - filter_height) times. At each of these positions / slides it will compute the dot product between the filter and the patch of the input. To compute the dot product we multiply each filter position with the corresponding input position. Then we sum all of those multiplications up. After calculating the dot product we add the bias. Then we apply our non-linearity (relu) to it and put it in the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e299d0f-9530-4cb7-8a2f-f371517bf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's implement the convolutional layer and max /average pooling layer and see how those do\n",
    "# if this becomes too slow, and I realize now that I may need to do a torch implementation because this autograd engine\n",
    "# is very slow lol. I guess because I'm using python lists and non-vectorized code everywhere\n",
    "\n",
    "class Convolutional_Unit(Module):\n",
    "    def __init__(self, kernel_height, kernel_width, **kwargs):\n",
    "        self.kernel_width = kernel_width\n",
    "        self.kernel_height = kernel_height\n",
    "        self.kernel = []\n",
    "        for i in range(kernel_height):\n",
    "            self.kernel.append([Value(random.uniform(-1,1)) for _ in range(kernel_width)])\n",
    "        self.b = Value(0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out_height = len(x) - self.kernel_height\n",
    "        out_width = len(x[0]) - self.kernel_width\n",
    "        # slide it over the input, return a 2d list of values\n",
    "        out = []\n",
    "        for i in range(out_height):\n",
    "            row = []\n",
    "            for j in range(out_width):\n",
    "                # quadrouple nested for loop lol\n",
    "                # there's definitely a lot ofrepeated work here, maybe I should make a map of values later and store them lol\n",
    "                # or just vectorize the code lol\n",
    "                acc = Value(0)\n",
    "                for kernel_i in range(kernel_height):\n",
    "                    for kernel_j in range(kernel_width):\n",
    "                        acc += self.kernel[kernel_i][kernel_j] * x[i + kernel_i][j + kernel_j]\n",
    "                acc += self.b\n",
    "                row.append(acc)\n",
    "            out.append(row)\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.kernel + [self.b]\n",
    "\n",
    "class Convolutional_Feature_Map(Module):\n",
    "    def __init__(self, num_units, kernel_height, kernel_width, **kwargs):\n",
    "        self.convolutional_units = [Convolutional_Unit(kernel_height, kernel_width) for _ in range(num_units)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [c(x) for c in self.convolutional_units]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for c in self.convolutional_units for p in c.parameters()]\n",
    "    \n",
    "class Average_Pooling_Layer(Module):\n",
    "    def __init__(self, window_height, window_width, stride_height, stride_width, nonlinearity):\n",
    "        self.window_width = kernel_width\n",
    "        self.window_height = kernel_height\n",
    "        self.stride_height = stride_height\n",
    "        self.stride_width = stride_width\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # for simplicity we have to ensure that the input size % stride == 0\n",
    "        if(len(x) % self.stride_height != 0 or len(x[0]) % self.stride_width != 0:\n",
    "            print(\"need to make it divisible bruh\")\n",
    "            return\n",
    "        out_height = len(x) // self.stride_height\n",
    "        out_width = len(x[0]) // self.stride_width\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                # HEMMINGWAY BRIDGE\n",
    "                # FINISH IMPLEMENTING THE AVERAGE POOLING HERE\n",
    "                \n",
    "        \n",
    "        \n",
    "# class Max_Pooling_Layer(Module):\n",
    "\n",
    "        \n",
    "# we'll also need some sort of class that can wrap things and chain them together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2b21b-83fb-49e6-8c7b-2d5f8152ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out how our new convolutional neural net learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df1328-f125-47da-b1f0-53f27f3258e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make better error metrics / reporting and make it more professional, train test set, charts, validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09413d04-d5dd-4459-8305-788d446d527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a bit of hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfde25-6d2e-4c9b-8472-b84efd74d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cap it here, go through appendecies to see if there's other things to understand\n",
    "# test the pre-condition for faster convergance and scale the tanh\n",
    "# do stochastic vs batch gradient descent\n",
    "# try their stochastic diagonal levenberg-marquardt / understand that"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML Learning)",
   "language": "python",
   "name": "ml_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
