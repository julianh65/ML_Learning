{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c112e4-4f90-494a-9e1f-50545259c3e9",
   "metadata": {},
   "source": [
    "### Introduction Notes\n",
    "We are interested in minimizing the error of our function over the test set. The gap between the test set and train set is given approximately as:\n",
    "\n",
    "$E_{\\text{test}} - E_{\\text{train}} = k(h/P)^{\\alpha}$\n",
    "\n",
    "If possible we'd like this term to be lower.\n",
    "\n",
    "P is the number of training examples, so increasing P decreases this gap.\n",
    "h is a measure of the capacity, so as the capacity goes up this value goes up, which makes sense as it will likely have more capacity to overfit\n",
    "\n",
    "In practice this is minimized by minimizing $E_{\\text{train}} + \\beta H (W)$ which is just where we add regularization.\n",
    "\n",
    "---\n",
    "\n",
    "We minimize this by computing the gradients and then updating the weights based on these gradients.\n",
    "\n",
    "Popular procedure is SGD or the \"on-line update\" where we don't get the complete gradient of the training set, but we get the gradient on the basis of a single or small amount of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e4f41-b4bb-4a70-a490-71d19ffd5f06",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "One advantage of convolutional neural networks is that they are better at handling invariance with respect to translations or local distortions. This is because with a large MLP, it could learn the different features in different positions, but it would be very inefficient as it would need multiple little mini networks within the network to handle detecting the important patterns positioned at different locations.\n",
    "\n",
    "Another issue with fully connected MLPs is that there is kind of this loss of information. The variables can be fed in in any fixed order and the outcome won't change. The information going into an MLP is just a 1d vector of numbers. Yet obviously images have a strong and important 2D structure.\n",
    "\n",
    "Convolutional networks use \"local receptive fields\" or patches. They extract the elementary features from them such as edges, corners etc... These features are then combined by subsequent convolutional layers in order to detect higher-order features.\n",
    "\n",
    "Because feature detectors that are useful on one part of the image are likely to be useful for the entire image, we use the single patch that has one set of weights to go across the entire image.\n",
    "\n",
    "Units in a layer are organized into planes where all the units share the same set of weights. What this means essentially is that we can kind of think of it as sliding a window of the same weights across the input. This would be opposed to, maybe in the bottom right we have a set of weights that detects a corner here and another set of weights in the top left that detects this corner here.\n",
    "\n",
    "This does not mean that each individual feature map has the same set of weights across them. That would be quite duplicative for no reason.\n",
    "\n",
    "In LeNet-5 the first layer has 6 planes or feature maps. A unit in a feature map has 25 inputs connected to a 5x5 area in the input. A sequential implementation (scanning across) would slide this 5x5 set of weights across the input and multiply each by the weight and then add the bias and squash at the end.\n",
    "\n",
    "Once a feature is detected, the location becomes less important, what's important is the position relative to other features.\n",
    "\n",
    "To reduce the precision with which the positions are encoded there is sub-sampling of layers which they use averaging but you could also use max pooling or something. This helps to reduce sensitivity to shifts and distortions, also may help with overfitting? Not sure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f9f6d-1a33-46d8-b659-4f730037d28d",
   "metadata": {},
   "source": [
    "Before I go into implementing CNNs I want to rework my code from the previous backprop notebook.\n",
    "\n",
    "I'm going to, based on Karpathy's micrograd, quickly write up / copy over the code so that I can have an autograd engine that implements .backwards. From there I'll add code on top of this that implements the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa616d-40ad-4fce-8df3-4692248a49b4",
   "metadata": {},
   "source": [
    "Because I want to practice this, I used o3 to turn the implementation of Value into an assignment with methods that I should implement myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "107be411-a995-4d9e-a73a-9a5c907749c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from typing import Iterable, Union\n",
    "\n",
    "Number = Union[int, float]\n",
    "\n",
    "\n",
    "class Value:\n",
    "    \"\"\"A node in a dynamically‑built computation graph.\"\"\"\n",
    "\n",
    "    # -------------------------------------------------------------- T1\n",
    "    def __init__(self, data: Number, _children: Iterable[\"Value\"] = (), _op: str = \"\"):\n",
    "        \"\"\"Create a **leaf** (when ``_children`` is empty) or an **operation\n",
    "        result** (when created inside an operator).\n",
    "\n",
    "        What this method must do\n",
    "        ------------------------\n",
    "        1. Store ``data`` – the wrapped scalar (float/int).\n",
    "        2. Create ``grad`` and set it to **0.0**.  We accumulate partial\n",
    "           derivatives here during back‑prop.\n",
    "        3. Track the immediate predecessors with ``self._prev = set(_children)``.\n",
    "           We use a **set** so look‑ups are O(1) and there are no duplicates.\n",
    "        4. Save ``_op`` – a short string describing *how* this node was\n",
    "           produced (\"+\", \"*\", \"ReLU\", etc.).  It is *only* for graphviz /\n",
    "           debugging convenience.\n",
    "        5. Initialise ``self._backward`` to a **do‑nothing** lambda.  Each\n",
    "           operator will later *replace* this with a closure that knows\n",
    "           how to distribute ``out.grad`` to its parents.\n",
    "\n",
    "        Why these pieces?\n",
    "        -----------------\n",
    "        * Automatic differentiation works by walking the graph from the\n",
    "          output back to the leaves, so every node needs a list of its\n",
    "          parents (``_prev``) **and** the rule to push its gradient back\n",
    "          (``_backward``).\n",
    "        * ``grad`` starts at 0 so multiple downstream paths can safely\n",
    "          *accumulate* into it.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self.prev = set(_children)\n",
    "        self._op = _op\n",
    "\n",
    "    def __add__(self, other: Union[\"Value\", Number]):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other: Union[\"Value\", Number]):\n",
    "        \"\"\"self * other.\"\"\"\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    # -------------------------------------------------------------- T4\n",
    "    def __pow__(self, other: Number):\n",
    "        \"\"\"self ** other (scalar exponent).\"\"\"\n",
    "        out = Value(self.data ** other, (self,), '**')\n",
    "        def _backward():\n",
    "            self.grad += (other) * self.data ** (other - 1)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "\n",
    "    # -------------------------------------------------------------- T5\n",
    "    def relu(self):\n",
    "        \"\"\"ReLU activation: max(0, x).\"\"\"\n",
    "        out = Value(max(self.data, 0), (self,), \"relu\")\n",
    "        def _backward():\n",
    "            self.grad += max(0, out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # -------------------------------------------------------------- T6\n",
    "    def backward(self):\n",
    "        \"\"\"Compute ``d(output)/d(node)`` for *every* ``node`` that influences\n",
    "        this Value (call it *out*).\n",
    "\n",
    "        Behaviour overview\n",
    "        ------------------\n",
    "        *The chain rule tells us we must process nodes **in reverse\n",
    "        topological order** – children **before** parents – so that when\n",
    "        we reach a node, the gradients flowing into it from all of its\n",
    "        consumers have already been accumulated.*\n",
    "\n",
    "        Implementation recipe\n",
    "        ---------------------\n",
    "        1. **Topological sort**\n",
    "           Depth‑first search starting from ``self`` collects nodes in a\n",
    "           list ``topo`` such that parents appear **before** children.\n",
    "        2. **Seed the output**\n",
    "           A node’s gradient with respect to itself is 1, so set\n",
    "           ``self.grad = 1.0``.\n",
    "        3. **Reverse sweep**\n",
    "           Iterate ``for v in reversed(topo): v._backward()``.  Each\n",
    "           stored ``_backward`` closure adds its *local* contribution to\n",
    "           ``child.grad``.\n",
    "        \"\"\"\n",
    "        # so we start from my node and go backwards calling backwards on all nodes from\n",
    "        # root me, all the way to the leaves\n",
    "\n",
    "        self.grad = 1\n",
    "        topographical_ancestors = []\n",
    "        seen = set()\n",
    "        def recurse(node):\n",
    "            topographical_ancestors.append(node)\n",
    "            seen.add(node)\n",
    "            if(node.prev):\n",
    "                for node in node.prev:\n",
    "                    if(node not in seen):\n",
    "                        recurse(node)\n",
    "        recurse(self)\n",
    "        print(topographical_ancestors)\n",
    "        for node in topographical_ancestors:\n",
    "            node._backward()\n",
    "            \n",
    "\n",
    "    # -------------------------------------------------------------- T7 helpers\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other: Union[\"Value\", Number]):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other: Union[\"Value\", Number]):\n",
    "        return other + (-self)\n",
    "\n",
    "    # -------------------------------------------------------------- T8\n",
    "    def __truediv__(self, other: Union[\"Value\", Number]):\n",
    "        return self * other ** -1\n",
    "\n",
    "    def __rtruediv__(self, other: Union[\"Value\", Number]):\n",
    "        return other * self ** -1\n",
    "\n",
    "    # -------------------------------------------------------------- T9\n",
    "    def __repr__(self):\n",
    "        return str(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0b59943-f7f8-451b-be47-250c3597a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.0, 6.0, 3.0, 2.0, 4.0]\n",
      "[2.5, 0, -1.0, 2.5, 2.5]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m test_relu()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# test_division()\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mtest_chain_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests passed ✨\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 50\u001b[0m, in \u001b[0;36mtest_chain_complex\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m y \u001b[38;5;241m=\u001b[39m Value(\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     49\u001b[0m z \u001b[38;5;241m=\u001b[39m Value(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m u \u001b[38;5;241m=\u001b[39m (\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m)\u001b[38;5;241m.\u001b[39mrelu() \u001b[38;5;241m*\u001b[39m z\n\u001b[1;32m     51\u001b[0m u\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# manual gradients\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# inside relu: r = x - y/z = 4 - 0.5 = 3.5 (>0) so relu grad = 1\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# u = r * z\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# du/dx = 1 * z = 2\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# du/dy = (-1/z) * z = -1\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# du/dz = r + (-y/z^2)*z = r - y/z = 3.5 - 0.5 = 3.0\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 132\u001b[0m, in \u001b[0;36mValue.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m, Number]):\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mother\u001b[49m)\n",
      "Cell \u001b[0;32mIn[20], line 129\u001b[0m, in \u001b[0;36mValue.__neg__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__neg__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "Cell \u001b[0;32mIn[20], line 55\u001b[0m, in \u001b[0;36mValue.__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m, Number]):\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"self * other.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     out \u001b[38;5;241m=\u001b[39m Value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m \u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m, (\u001b[38;5;28mself\u001b[39m, other), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m():\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m out\u001b[38;5;241m.\u001b[39mgrad\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "from math import isclose\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "def close(a, b, eps=EPS):\n",
    "    assert isclose(a, b, rel_tol=0.0, abs_tol=eps), f\"{a} != {b}\"\n",
    "\n",
    "\n",
    "def test_add_mul_pow():\n",
    "    # z = x * y + x ** 2\n",
    "    x = Value(2.0)\n",
    "    y = Value(3.0)\n",
    "    z = x * y + x ** 2\n",
    "    z.backward()\n",
    "\n",
    "    # forward value\n",
    "    close(z.data, 2.0 * 3.0 + 2.0 ** 2)  # 10.0\n",
    "\n",
    "    # analytic grads\n",
    "    close(x.grad, y.data + 2 * x.data)   # 3 + 4 = 7\n",
    "    close(y.grad, x.data)               # 2\n",
    "    close(z.grad, 1.0)\n",
    "\n",
    "\n",
    "def test_relu():\n",
    "    a = Value(-1.0).relu()\n",
    "    b = Value(2.5).relu()\n",
    "    out = a + b\n",
    "    out.backward()\n",
    "\n",
    "    close(a.data, 0.0)\n",
    "    close(b.data, 2.5)\n",
    "\n",
    "def test_division():\n",
    "    a = Value(3.0)\n",
    "    b = Value(2.0)\n",
    "    c = a / b  # shorthand for a * b ** -1\n",
    "    c.backward()\n",
    "\n",
    "    close(c.data, 1.5)\n",
    "    close(a.grad, 1 / b.data)              # 0.5\n",
    "    close(b.grad, -a.data / b.data ** 2)   # -0.75\n",
    "\n",
    "\n",
    "def test_chain_complex():\n",
    "    # u = (x - y / z).relu() * z\n",
    "    x = Value(4.0)\n",
    "    y = Value(1.0)\n",
    "    z = Value(2.0)\n",
    "    u = (x - y / z).relu() * z\n",
    "    u.backward()\n",
    "\n",
    "    close(x.grad, 2.0)\n",
    "    close(y.grad, -1.0)\n",
    "    close(z.grad, 3.0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # simple CLI run without pytest\n",
    "    test_add_mul_pow()\n",
    "    test_relu()\n",
    "    # test_division()\n",
    "    test_chain_complex()\n",
    "    print(\"All tests passed ✨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d00b0-dadb-49ec-805b-00e95936723a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1736bf39-9b59-4f4d-94ce-84c02ea841b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reimplement Karpathy's setup from micrograd to create NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99968460-9f09-443f-a1d4-c0eba247fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test micrograd on simple excercise from past paper like the adder problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bdbe4-9a29-4dea-914c-857b28000378",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up yann le cunn's loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1cd7ac-021b-44c9-bca5-8d4bc3537362",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test running just a fully connected network on mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc6b02-7de9-401b-961d-e07ce081488f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e299d0f-9530-4cb7-8a2f-f371517bf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the CNN on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277a492-e443-498b-b303-0686e6113c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## try augmenting the data and seeing what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfde25-6d2e-4c9b-8472-b84efd74d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add some regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
