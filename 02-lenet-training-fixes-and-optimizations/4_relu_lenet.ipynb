{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8688c7-2a82-4dc5-aa1e-5950ca45e37c",
   "metadata": {},
   "source": [
    "# Notes\n",
    "Did my first read of the paper. It's super interesting and well written and has a lot of nice insights and data. I enjoyed how it also presented some reasons as to why ReLU works from a neuroscience / biological perspective.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The relu activation function or max(0,x) is interesting because it both has better performance but is also quite similar to our biological neurons. In the paper they use relu + an L1 regularizer to promote sparsity, which they claim have many nice effects / properties for the network.\n",
    "\n",
    "### Background\n",
    "\n",
    "There are several phenomena / observations of how the brain works that are similar to the types of network that using Relu activation functions creates. Firstly the brain has sparse activations. It's estimated that between 1-4% of neurons in the brain are activate at one time. With sigmoid, when it receives a zero input (it's steady state), we get an output of around 1/2. This means that upon initialization, on average, all neurons are firing at 1/2. Additionally biological models of neuron activations have a threshold to fire, and below that threshold they don't fire. Relu has that threshold where it just doesn't fire, at < 0.\n",
    "\n",
    "Sparsity in networks has some claimed advantages. It allows for more disentangled \"clean\" models. This seemingly makes the models more robust as with highly entangled models where everything is activating everything, a small change in the input can modify weights / activations in the entire model. In a sparse model, small changes in the input affect less of the entire model.\n",
    "\n",
    "Sparse representations are more likely to be linearly seperable. This makes sense to me. When we have kind of a wide layer we're projecting the data into a higher-dimensional space. But if all the points in that higher dimensional space need to be considered then it becomes kind of a challenge, likely what will happen is that it becomes overfit, where our linearly seperable bound becomes some weird contorted shape to fit everything. If we make it spare however, then when we go to that higher dimensional space, we have less unique points to fit as a bunch will just be 0s. So we get the more useful features in that space and then have to fit that. So it's kind of like a regularization?\n",
    "\n",
    "It's important to not go too hard on sparsity as it could affect the performance and reduce the effective capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678cef5-0ca3-4674-bf7b-9ad2c30fec30",
   "metadata": {},
   "source": [
    "### Deep Rectifier Networks\n",
    "\n",
    "What's nice about relu is that after uniform initialization of weights we can expect around 50% of units to output 0s. The non-linearity in the network comes from the path selection. For a given input the computation is actually linear. The network is linear by parts. Gradients flow nicer and there's no vanishing gradient problem.\n",
    "\n",
    "One of the potential problems listed is that \"the hard saturation at 0 may hurt optimization by blocking gradient back-propogation\". This means that some may think that it's a problem that the gradient just dies and stops flowing at 0. But they test a similar looking smooth version soft plus and see that the hard 0 actually can help.\n",
    "\n",
    "Another issue is that because the activation is unbounded, in that the limit is infinity, so we may get some huge numbers. To combat this we use an L1 penalty / regularization.\n",
    "\n",
    "Note: At 0 the function is technically not differentiable, so in practice if the activation is 0 then we set the gradient to 0.\n",
    "\n",
    "Side note: I wonder if it's computationally more efficient as well. Gradient is easier to compute and also the value is easier to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1dd377-ff02-498d-8852-80bb4b5f39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare tanh, relu, sigmoid and softplus on same training I had from lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0b3a6-7c4c-44da-9eed-67bd5cc2d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare them with an l1 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dde130-391b-41d1-9662-5261ee8da5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some way to track sparsity and print it out to see how the sparsity emerges in relu networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
