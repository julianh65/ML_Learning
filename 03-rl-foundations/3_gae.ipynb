{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5089ca",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "### High Level Quick Read Through\n",
    "\n",
    "So this isn't fundamentally a new setup or architecture, this is an improvement on the way that we calculate the returns for what we use when we compute the advantages. Let's go through a bit of background of what's going on and frame the problem and then show how GAE is better.\n",
    "\n",
    "So the context is that in RL we want to find a policy that maximizes the expected cumulative reward. The objective function J(theta) is the expected return over all possible trajectories that could be generated by our policy. Our goal is to find theta such that it maximizes this value. Theta is the the params. To maximize J we perform gradient ascent on theta. The challenging part is calculating the gradient.\n",
    "\n",
    "The policy gradient theorem gives us a way to compute this. The general / common form is:\n",
    "\n",
    "$$\n",
    "g = \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\Psi_t \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\right] \\quad\n",
    "$$\n",
    "\n",
    "Where delta log pi term is the score function. This is a vector (of the same shape of our weights theta) of the gradient of the log probability with respect to the policys parameters. We then have Psi which acts as the magnitude and sign for our update. This in A2C is the advantage that we calculate.\n",
    "\n",
    "This paper is about finding a good choice for Psi. First they go through the different existing and flawed choices for psi.\n",
    "\n",
    "1. The total reward of the trajectory\n",
    "This means that we sum up the total reward so far for the trajectory. This has a few different flaws. The major one is the causality issue. Good or bad actions that happened in the past could cause this current action to be reinforced regardless if this action was good or bad. This would cause the updates to be very high variance, making the learning slow and unstable. \n",
    "\n",
    "\n",
    "2. The reward following action a_t\n",
    "This is a bit better. Instead of for each one sum up the reward you look at the reward that you got from now until the future. So if you're looking at calculating psi for timestep 3 you would sum the reward for timestep 3 until the end of the episode. This helps to assign credit a bit better\n",
    "\n",
    "3. Using a baseline\n",
    "This is a trick that we use to reduce variance. We can subtract any function b(s_t) that depends only and only on the state s (and not the action you take) from our reward-to-go without changing the expected value of the gradient (that is to say introducing bias). This makes it more stable. A good baseline would be the avreage value of the state (which is the value function!)\n",
    "\n",
    "4. Q- Function\n",
    "This is another valid choice. The only issue is that it still suffers from high variance. For example in a state where all actions are good, all Q values will be large and positive and give a big gradient to reinforce any action in that state. It doesn't give a strong signal about which action is better than others.\n",
    "\n",
    "5. Advantage Function\n",
    "This is the best possible choice for psi from a variance perspective. It is the reward to go with the optimal baseline subtracted. The learning signal is centered around zero. Thisi s a much cleaner signal for learning. This paper is actually about finding the best way to estimate this quantity.\n",
    "\n",
    "6. TD Residual\n",
    "The TD residual is the one-step estimate of the advantage function. Because we don't have the perfect advantage function (what the critic is trying to learn), we can reduce the bias of it by increasing the variance by adding in the rewards of a certain number of time steps.\n",
    "\n",
    "### The core idea of the paper -- generalized advantage estimation\n",
    "\n",
    "The main issue is that we don't know the true value of the value function. We learn it with a network (critic network). The critic network is not going to be perfect obviously, which will introduce bias. The rest of the paper talks about managing the tradeoff between the tradeoff that is introduced by using empirical returns and the bias of using the value function to calculate the advantage.\n",
    "\n",
    "The simplest possible advantage estimator uses V heavily. It's the TD residual or TD error. Note: this is the value that we use as psi or the advantage I suppose.\n",
    "\n",
    "$$\n",
    "\\delta_t^V = r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t) \\quad\n",
    "$$\n",
    "\n",
    "This value is defined as the reward we get at a timestep plus gamma multiplied by what our value function estimates at the next step minus the value function at this step. I can see why this uses V very heavily. It might work but will be quite biased. At early parts of the training it will be very biased (probably incorrectly) as the value network hasn't really learned anything yet. \n",
    "\n",
    "This is low variance though. It's low variance because it's composed of fewer random variables than the alternative monte carlo return which we'll cover in a second. Note: variance here refers to how much their value fluctuates for the same state-action pairs across different trajectories. Monte carlo is random because it's a function of a long chain of random events, a single different action at a step in the future can lead to a completely different sub trajectory. It's high bias because it relies heavily on the probably inacurate value function.\n",
    "\n",
    "\n",
    "The next idea to look at is using k-step estimators. This is similar to n-step returns and is inbetween TD residual and monte carlo estimate. They define a family of estimators that look k steps into the future. This is different to n-step return in that n-step return is a target for the value function. The k-step advantage estimator is a weight for the policy gradient.\n",
    "\n",
    "This is the formula for it:\n",
    "\n",
    "$$\n",
    "\\hat{A}_t^{(k)} = \\sum_{l=0}^{k-1} \\gamma^l \\delta_{t+l}^V\n",
    "$$\n",
    "\n",
    "This is essentially summing the TD error (which we defined before) kind of \"recursively\" down different time steps. For k = 2 we would have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{A}_t^{(2)} &= \\delta_t^V + \\gamma \\delta_{t+1}^V \\\\\n",
    "&= \\underbrace{(r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t))}_{\\delta_t^V} + \\gamma \\underbrace{(r_{t+1} + \\gamma V_\\phi(s_{t+2}) - V_\\phi(s_{t+1}))}_{\\delta_{t+1}^V} \\\\\n",
    "&= r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t) + \\gamma r_{t+1} + \\gamma^2 V_\\phi(s_{t+2}) - \\gamma V_\\phi(s_{t+1})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and if we look closely the two terms in the middle cancel out the gramma V (s_t+1) and minus gamma V (st+1) at the end. These will continue to cancel leaving us with something like this.\n",
    "\n",
    "$$\n",
    "\\hat{A}_t^{(k)} = \\underbrace{\\left( \\sum_{l=0}^{k-1} \\gamma^l r_{t+l} \\right)}_{\\text{k-step empirical return}} + \\underbrace{\\gamma^k V_\\phi(s_{t+k})}_{\\text{bootstrap value}} - \\underbrace{V_\\phi(s_t)}_{\\text{baseline}}\n",
    "$$\n",
    "\n",
    "Which is the general form. Let's look at each of the parts one by one. So the first is part 3, the basline. That's what our critic network thinks is the expectation of the total discounted rewards for this state. We subtract this to center our calculation. The k-step return part 1 is the ground truth portion of our estimate. It's the actual discounted sum of rewards that we observed. The last part is our bootstrap value, that's the estimation part of our new estimate. That's what our network predicts the value of that state is after taking those k steps (we discount it by gamma ^k)\n",
    "\n",
    "Now, what does the generalized advantage estimator do? Instead of picking one value of k in the above formula it combines all of them using an exponentially-weighted aveerage that is controlled by a new hyperparameter lambda E [0,1].\n",
    "\n",
    "The definition is\n",
    "\n",
    "$\\hat{A}_t^{\\text{GAE}} = (1-\\lambda) \\sum_{k=1}^\\infty \\lambda^{k-1} \\hat{A}_t^{(k)}$\n",
    "\n",
    "Which is 1 - lambda multiplied by the sum of weighting the above defined advantage functions for k steps by lambda ^ k-1.\n",
    "\n",
    "This simplifies to the the following formula which is a lot more simple:\n",
    "\n",
    "$$\n",
    "\\hat{A}_t^{\\text{GAE}(\\gamma, \\lambda)} = \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V \\quad (\\text{Equation 16})\n",
    "$$\n",
    "\n",
    "This equation is discounted sum of all future TD residuals. For every step we calculate (and sum) the surprise score (td residual defined earlier as reward we got + gamma value of next situation - value of current situation). So before we do anything else we go through all the steps into the future and calculate that for each state.\n",
    "\n",
    "Then we need to look at the sum and what the l is. The l stands for lookahead, it's how many steps into the future we are looking at from our current state / position t=0. So what this is defined as then is lambda (our new hyper parameter) multiplied by gamma (our discount factor) to the power of the l (for exponential weighting) multiplied by the TD residual at that step.\n",
    "\n",
    "Obivously this is just math, in reality we can't do an infinite sum. In practice two things are done. One is that our episodes are finite and end (depending on the setup). If that's the case we can sum til the end. In other cases we collect data in finite batches / finite amounts of steps. The sum stops at the end of our collected data.\n",
    "\n",
    "We have a clever trick with this formula. We can efficiently calculate it with a backward pass. If we look at A_t it contains the definition of A_t+1 in it. Therefore this simplifies to \n",
    "\n",
    "\n",
    "$A_t = \\delta_t + \\gamma \\lambda \\cdot A_{t+1}$\n",
    "\n",
    "Now we have to look at gamma and lambda. They seem to do similar things so it's important to differenciate them.\n",
    "\n",
    "Gamma is still that same discount factor, it defines how much we care about future rewards. Higher gamma means we have a long horizon, lower means we care more about immediate rewards.\n",
    "\n",
    "Lambda controls the bias-variance tradeoff of the advantage estimator. It doesn't change the definition of the value function. A higher lambda will mean that we take into account future estimations at a higher weight (making it more similar to monte carlo). This will reduce the bias but increase the variance. At lambda = 0 then we get high bias low variance when the value function is good.\n",
    "\n",
    "Note: I wonder if there's anything about decaying lambda as you go on?\n",
    "\n",
    "Comparing to N_Step Advantage\n",
    "\n",
    "1. N step advantage\n",
    "    advantage is the outcome over n real steps minus the baseline\n",
    "    we go to the end of the episode and the boostrap value and then work backwards\n",
    "    discounting and adding in the real rewards\n",
    "2. GAE\n",
    "    Advantage is exponentially weighted sum of all future td residuals\n",
    "    do the backwards pass and for each state we first calculate the td residuals which is defined\n",
    "    as the reward we got now + the discounted value of the next step - the value of this step\n",
    "    then with those td residuals we sum them over time\n",
    "\n",
    "\n",
    "So what is the actual difference I have to make in my A2C implementation to implement this? Instead of using N-Step advantage we replace it with the GAE \n",
    "\n",
    "Note on implementation: there isn't a way to manually truncate GAE sum to a fixed number of steps, it always goes to the end of the rollout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ff49e",
   "metadata": {},
   "source": [
    "### Quick Implementation Plan\n",
    "1. Take my previous A2C algorithm and replace the n-step return with GAE in order to calculate the advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011041d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic-control] in /Users/julianhunt/Desktop/ML_Learning/venv/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/julianhunt/Desktop/ML_Learning/venv/lib/python3.12/site-packages (from gymnasium[classic-control]) (2.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/julianhunt/Desktop/ML_Learning/venv/lib/python3.12/site-packages (from gymnasium[classic-control]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/julianhunt/Desktop/ML_Learning/venv/lib/python3.12/site-packages (from gymnasium[classic-control]) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/julianhunt/Desktop/ML_Learning/venv/lib/python3.12/site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/julianhunt/Desktop/ML_Learning/venv/lib/python3.12/site-packages (from gymnasium[classic-control]) (2.6.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install \"gymnasium[classic-control]\"\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f5ddcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import threading\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, n_actions, hidden_size):\n",
    "        # initialize the shared body\n",
    "        # create the actor head and the critic head as their own extra linear layers on the side\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(4, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor_head = nn.Linear(hidden_size, 2)\n",
    "        self.critic_head = nn.Linear(hidden_size, 1)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.as_tensor(x, dtype=torch.float32, device=device)\n",
    "        shared_features = self.shared(x)\n",
    "        action_distribution = self.actor_head(shared_features)\n",
    "        critic_value = self.critic_head(shared_features)\n",
    "        return action_distribution, critic_value\n",
    "\n",
    "class Worker(threading.Thread):\n",
    "    def __init__(self, worker_id, task_queue, results_queue, actor_critic):\n",
    "        super().__init__()\n",
    "        self.worker_id = worker_id\n",
    "        self.task_queue = task_queue\n",
    "\n",
    "        # environment and agent attributes\n",
    "        self.gym_environment = gym.make(\"CartPole-v1\")\n",
    "        self.environment_state = self.gym_environment.reset()[0]\n",
    "        self.actor_critic = actor_critic\n",
    "        self.results_queue = results_queue\n",
    "    \n",
    "    def run(self):\n",
    "        while True:\n",
    "            command, data = self.task_queue.get()\n",
    "            if command == 'collect':\n",
    "                experience = self.collect_experience(data['n_steps'])\n",
    "                self.results_queue.put((self.worker_id, experience))\n",
    "            elif command == 'stop':\n",
    "                break\n",
    "            \n",
    "    def collect_experience(self, n_steps):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        terminates = False\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            with torch.no_grad():\n",
    "                action_distribution_logits, critic_estimate = self.actor_critic.forward(self.environment_state)\n",
    "            log_probabilities = F.log_softmax(action_distribution_logits, dim=-1)\n",
    "            probs = torch.exp(log_probabilities)\n",
    "            action = torch.multinomial(probs, num_samples=1).item()\n",
    "            next_state, reward, terminated, truncated, info = self.gym_environment.step(action)\n",
    "            \n",
    "            states.append(self.environment_state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(terminated)\n",
    "\n",
    "            self.environment_state = next_state\n",
    "\n",
    "            if(terminated):\n",
    "                self.environment_state = self.gym_environment.reset()[0]\n",
    "                terminates = True\n",
    "                break\n",
    "                \n",
    "        last_state = self.environment_state\n",
    "                    \n",
    "        return {\n",
    "            \"states\": states,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"advantages\": self.calculate_GAE(states, last_state, rewards, terminates),\n",
    "            }     \n",
    "\n",
    "    def calculate_GAE(self, states, last_state, rewards, terminates):\n",
    "        lambda_val = 0.95\n",
    "        gamma_val = 0.99\n",
    "        # we need to go from backwards to front, we first get the bootstrap value\n",
    "        # from the last state\n",
    "        # we then first calculate all the td residuals\n",
    "        a, prev_value = self.actor_critic.forward(last_state)\n",
    "        if(terminates):\n",
    "            prev_value = 0\n",
    "        td_residuals = torch.zeros(len(states))\n",
    "        for i in reversed(range(len(states))):\n",
    "            a, curr_value = self.actor_critic.forward(states[i])\n",
    "            td_residuals[i] = rewards[i] + gamma_val * prev_value - curr_value\n",
    "            prev_value = curr_value\n",
    "\n",
    "        previous_advantage = 0\n",
    "        advantages = torch.zeros(len(states))\n",
    "        for i in reversed(range(len(states))):\n",
    "            advantages[i] = td_residuals[i] + lambda_val * gamma_val * previous_advantage\n",
    "            previous_advantage = advantages[i]\n",
    "        \n",
    "        return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f2d5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Episode 5\n",
      "Episode 10\n",
      "Episode 15\n",
      "Episode 20\n",
      "Episode 25\n",
      "Episode 30\n",
      "Episode 35\n",
      "Episode 40\n",
      "Episode 45\n",
      "Episode 50\n",
      "Episode 55\n",
      "Episode 60\n",
      "Episode 65\n",
      "Episode 70\n",
      "Episode 75\n",
      "Episode 80\n",
      "Episode 85\n",
      "Episode 90\n",
      "Episode 95\n",
      "Episode 100\n",
      "Episode 105\n",
      "Episode 110\n",
      "Episode 115\n",
      "Episode 120\n",
      "Episode 125\n",
      "Episode 130\n",
      "Episode 135\n",
      "Episode 140\n",
      "Episode 145\n",
      "Episode 150\n",
      "Episode 155\n",
      "Episode 160\n",
      "Episode 165\n",
      "Episode 170\n",
      "Episode 175\n",
      "Episode 180\n",
      "Episode 185\n",
      "Episode 190\n",
      "Episode 195\n",
      "Episode 200\n",
      "Episode 205\n",
      "Episode 210\n",
      "Episode 215\n",
      "Episode 220\n",
      "Episode 225\n",
      "Episode 230\n",
      "Episode 235\n",
      "Episode 240\n",
      "Episode 245\n",
      "Episode 250\n",
      "Episode 255\n",
      "Episode 260\n",
      "Episode 265\n",
      "Episode 270\n",
      "Episode 275\n",
      "Episode 280\n",
      "Episode 285\n",
      "Episode 290\n",
      "Episode 295\n",
      "Episode 300\n",
      "Episode 305\n",
      "Episode 310\n",
      "Episode 315\n",
      "Episode 320\n",
      "Episode 325\n",
      "Episode 330\n",
      "Episode 335\n",
      "Episode 340\n",
      "Episode 345\n",
      "Episode 350\n",
      "Episode 355\n",
      "Episode 360\n",
      "Episode 365\n",
      "Episode 370\n",
      "Episode 375\n",
      "Episode 380\n",
      "Episode 385\n",
      "Episode 390\n",
      "Episode 395\n",
      "Episode 400\n",
      "Episode 405\n",
      "Episode 410\n",
      "Episode 415\n",
      "Episode 420\n",
      "Episode 425\n",
      "Episode 430\n",
      "Episode 435\n",
      "Episode 440\n",
      "Episode 445\n",
      "Episode 450\n",
      "Episode 455\n",
      "Episode 460\n",
      "Episode 465\n",
      "Episode 470\n",
      "Episode 475\n",
      "Episode 480\n",
      "Episode 485\n",
      "Episode 490\n",
      "Episode 495\n",
      "Episode 500\n",
      "Episode 505\n",
      "Episode 510\n",
      "Episode 515\n",
      "Episode 520\n",
      "Episode 525\n",
      "Episode 530\n",
      "Episode 535\n",
      "Episode 540\n",
      "Episode 545\n",
      "Episode 550\n",
      "Episode 555\n",
      "Episode 560\n",
      "Episode 565\n",
      "Episode 570\n",
      "Episode 575\n",
      "Episode 580\n",
      "Episode 585\n",
      "Episode 590\n",
      "Episode 595\n",
      "Episode 600\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# main training loop\n",
    "import queue\n",
    "print(\"starting\")\n",
    "\n",
    "# hyper params\n",
    "NUM_WORKERS = 6\n",
    "EPISODES = 600\n",
    "UPDATE_STEPS = 96\n",
    "LEARNING_RATE = 0.0003\n",
    "\n",
    "# setup the global network and optimizer\n",
    "global_network = ActorCritic(4, 128)\n",
    "optimizer = optim.Adam(global_network.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "workers = []\n",
    "task_queues = []\n",
    "results_queue = queue.Queue()\n",
    "episode_rewards = [] \n",
    "\n",
    "for i in range(NUM_WORKERS):\n",
    "    task_q = queue.Queue()\n",
    "    worker = Worker(i, task_q, results_queue, global_network)\n",
    "    workers.append(worker)\n",
    "    task_queues.append(task_q)\n",
    "    worker.start()\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    if (episode + 1) % 25 == 0:\n",
    "        print(f\"Episode {episode+1}\")\n",
    "\n",
    "    for q in task_queues:\n",
    "        q.put(('collect', {\"n_steps\": UPDATE_STEPS}))\n",
    "\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "    advantages = []\n",
    "\n",
    "    for _ in range(NUM_WORKERS):\n",
    "        worker_id, experience = results_queue.get()\n",
    "        all_states.extend(experience['states'])\n",
    "        all_actions.extend(experience['actions'])\n",
    "        advantages.extend(experience['advantages'])\n",
    "        episode_rewards.append(sum(experience['rewards']))\n",
    "\n",
    "    all_states = torch.tensor(all_states, dtype=torch.float32).to(device)\n",
    "    all_actions = torch.tensor(all_actions, dtype=torch.int64).to(device)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
    "\n",
    "    action_logits, critic_values = global_network(all_states)\n",
    "    all_critic_estimates = critic_values.squeeze()\n",
    "    critic_targets = advantages + all_critic_estimates.detach()\n",
    "\n",
    "    critic_loss = F.mse_loss(all_critic_estimates, critic_targets)\n",
    "\n",
    "    all_advantages = advantages.detach()\n",
    "\n",
    "    log_probs = F.log_softmax(action_logits, dim=-1)\n",
    "    probs = F.softmax(action_logits, dim=-1)\n",
    "    entropy = -(probs * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "    action_log_probs = log_probs.gather(1, all_actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "    policy_loss = -(action_log_probs * all_advantages).mean()\n",
    "\n",
    "    total_loss = policy_loss + 0.5 * critic_loss - 0.01 * entropy\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "for q in task_queues:\n",
    "    q.put(('stop', None))\n",
    "for w in workers:\n",
    "    w.join()\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08e16511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Evaluation episode finished!\n",
      "Total Reward: 386.0\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "eval_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "state, info = eval_env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    while not done:\n",
    "        eval_env.render()\n",
    "\n",
    "        action_logits, _ = global_network.forward(state)\n",
    "        \n",
    "        action_probs = F.softmax(action_logits, dim=-1)\n",
    "        \n",
    "        action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "\n",
    "eval_env.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"Evaluation episode finished!\")\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd98989",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML Learning)",
   "language": "python",
   "name": "ml_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
