# ML Paper Reimplementations: A Journey Through Foundational Concepts

This repository documents my self-study project focused on building a deep understanding of fundamental Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) concepts by reading and reimplementing seminal research papers.

**My Goal:** To prepare for grad school by moving beyond passive learning and actively engaging with the "why" and "how" of core algorithms.

**Methodology:**

- **Paper Selection:** Following a structured syllabus covering foundational to advanced topics.
- **Deep Dive:** Thoroughly reading each paper, supported by external resources and note-taking.
- **Implementation:** Implementing core algorithms from scratch.
- **Documentation:** Recording notes, insights, challenges, and code.
- **Reflection:** Writing blog posts (grouped by themes) to consolidate learning and share insights. [Link to Your Blog Here - e.g., My Learning Blog](https://yourblog.example.com)

## Current Progress & Syllabus

\_Status Legend: ✅ Done, ⏳ In Progress

---

### Phase 1: Foundations & Training Fundamentals

| Concept                          | Paper Name (Primary Reference)                          | Status |
| :------------------------------- | :------------------------------------------------------ | :----- |
| Backpropagation                  | Learning Representations by Back-propagating Errors     | ⏳     |
| Rectified Linear Units (ReLU)    | (General Concept / Glorot et al. 2011)                  |        |
| Adam Optimizer                   | Adam: A Method for Stochastic Optimization              |        |
| Dropout Regularization           | Dropout: A Simple Way to Prevent NNs from Overfitting   |        |
| Batch Normalization              | Batch Normalization: Accelerating Deep Network Training |        |
| Perceptron                       | The Perceptron: A Probabilistic Model... (Rosenblatt)   |        |
| Weight Decay (L2 Regularization) | (General Concept / Krogh & Hertz 1991)                  |        |

### Phase 2: Convolutional Networks for Vision

| Concept                              | Paper Name (Primary Reference)                          | Status |
| :----------------------------------- | :------------------------------------------------------ | :----- |
| Convolutional Neural Network (LeNet) | Gradient-Based Learning Applied to Document Recognition |        |
| Deep CNNs (AlexNet)                  | ImageNet Classification with Deep CNNs                  |        |
| Residual Learning (ResNet)           | Deep Residual Learning for Image Recognition            |        |
| Very Deep CNNs (VGG)                 | Very Deep Convolutional Networks for Large-Scale...     |        |
| U-Net Architecture                   | U-Net: Convolutional Networks for Biomedical Image Seg. |        |

### Phase 3: Sequence Models → Transformers

| Concept                                 | Paper Name (Primary Reference)                                                   | Status |
| :-------------------------------------- | :------------------------------------------------------------------------------- | :----- |
| Long Short-Term Memory (LSTM)           | Long Short-Term Memory (Hochreiter & Schmidhuber)                                |        |
| Sequence-to-Sequence Learning (Seq2Seq) | Sequence to Sequence Learning with Neural Networks                               |        |
| Attention Mechanism                     | Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau) |        |
| Transformer Architecture                | Attention Is All You Need                                                        |        |
| Bidirectional Encoders (BERT)           | BERT: Pre-training of Deep Bidirectional Transformers                            |        |
| Recurrent Neural Network (RNN)          | (General Concept / Williams 1989 for context)                                    |        |
| Word Embeddings (Word2Vec)              | Efficient Estimation of Word Representations...                                  |        |
| Vision Transformer (ViT)                | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale       |        |
| Mixture of Experts (MoE)                | Outrageously Large Neural Networks: The Sparsely-Gated MoE Layer (Shazeer)       |        |
| State Space Models (Mamba)              | Mamba: Linear-Time Sequence Modeling with Selective State Spaces                 |        |

### Phase 4: Generative Models

| Concept                                  | Paper Name (Primary Reference)                  | Status |
| :--------------------------------------- | :---------------------------------------------- | :----- |
| Variational Autoencoder (VAE)            | Auto-Encoding Variational Bayes                 |        |
| Generative Adversarial Network (GAN)     | Generative Adversarial Nets                     |        |
| Denoising Diffusion Probabilistic Models | Denoising Diffusion Probabilistic Models (DDPM) |        |
| Large Scale GANs (BigGAN)                | Large Scale GAN Training for High Fidelity...   |        |
| Autoregressive Audio (WaveNet)           | WaveNet: A Generative Model for Raw Audio       |        |

### Phase 5: Deep Reinforcement Learning

| Concept                                   | Paper Name (Primary Reference)                              | Status |
| :---------------------------------------- | :---------------------------------------------------------- | :----- |
| Deep Q-Network (DQN)                      | Human-level control through deep reinforcement learning     |        |
| Deep Deterministic Policy Gradient (DDPG) | Continuous control with deep reinforcement learning         |        |
| Soft Actor-Critic (SAC)                   | Soft Actor-Critic: Off-Policy Maximum Entropy...            |        |
| Policy Gradients / A2C                    | Asynchronous Methods for Deep Reinforcement Learning (A3C)  |        |
| RL + MCTS (AlphaGo/Zero)                  | Mastering the game of Go with deep neural networks...       |        |
| Protein Structure Prediction (AlphaFold)  | Highly accurate protein structure prediction with AlphaFold |        |

### Phase 6: Robotics Applications & Scaling Tricks

| Concept                              | Paper Name (Primary Reference)                               | Status |
| :----------------------------------- | :----------------------------------------------------------- | :----- |
| Vision-Based Robotic Grasping        | Learning Hand-Eye Coordination for Robotic Grasping          |        |
| Low-Rank Adaptation (LoRA)           | LoRA: Low-Rank Adaptation of Large Language Models           |        |
| Domain Randomization (Sim2Real)      | Domain Randomization for Transferring Deep NNs...            |        |
| Efficient Attention (FlashAttention) | FlashAttention: Fast and Memory-Efficient Exact Attention... |        |

---

This project is a work in progress. Feedback, suggestions, and discussions are welcome!
